var documenterSearchIndex = {"docs":
[{"location":"structs/data/#dataset","page":"Data Sets","title":"Data Sets","text":"","category":"section"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"Training (and testing data) are stored in a DataSet structure (with either scalar or vector valued y) ","category":"page"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"ARFF.ScalarDataSet\nARFF.VectorDataSet","category":"page"},{"location":"structs/data/#ARFF.ScalarDataSet","page":"Data Sets","title":"ARFF.ScalarDataSet","text":"ScalarDataSet{TC<:Complex,TR<:AbstractFloat,TW<:AbstractArray{TR},TI<:Integer}\n\nTraining data containing (x,y) data pairs stored in arrays of x values and arrays of y values.\n\nFields\n\nx - Array of real valued vectors \ny - Array of complex scalars\ny_mat - y represented as a matrix\nN - Number of data points\ndx - Dimension of x coordinates\n\n\n\n\n\n","category":"type"},{"location":"structs/data/#ARFF.VectorDataSet","page":"Data Sets","title":"ARFF.VectorDataSet","text":"VectorDataSet{TC<:Complex,TR<:AbstractFloat,TW<:AbstractArray{TR},TB<:AbstractArray{TC},TI<:Integer}\n\nTraining data containing (x,y) data pairs stored in arrays of x values and arrays of y values.\n\nFields\n\nx - Array of real valued vectors \ny - Array of complex valued vectors\ny_mat - y represented as a matrix\nN - Number of data points\ndx - Dimension of x coordinates\ndy - Dimension of y coordinates\n\n\n\n\n\n","category":"type"},{"location":"structs/data/#Constructing-Data-Sets","page":"Data Sets","title":"Constructing Data Sets","text":"","category":"section"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"Data sets can be constructed by passing in the (x_iy_i) information:","category":"page"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"    DataSet","category":"page"},{"location":"structs/data/#ARFF.DataSet","page":"Data Sets","title":"ARFF.DataSet","text":"DataSet(x, y)\n\nConstructor for a ScalarDataSet\n\nFields\n\nx - Array of real valued vectors \ny - Array of scalars\n\n\n\n\n\nDataSet(x, y)\n\nConstructor for a VectorDataSet\n\nFields\n\nx - Array of real valued vectors \ny - Array of vectors\n\n\n\n\n\n","category":"function"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"For instance, a scalar valued data set can be generated with: A training set can be constructed as follows:","category":"page"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"using ARFF\nusing Random \nRandom.seed!(100); # for reproducibility\n\nn = 10; # number of sample points\nx = [rand(2) for _ in 1:n];\nf(x) = exp(-x[1]*x[2]); # arbitrary function\ny = complex.(f.(x)); # make y valued data complex for type consistency\n\ndata = DataSet(x,y);\nprintln(data.x)\nprintln(data.y)","category":"page"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"Vector valued data can be similarly constructed:","category":"page"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"using ARFF\nusing Random \nRandom.seed!(100); # for reproducibility\n\nn = 10; # number of sample points\nx = [rand(2) for _ in 1:n];\nf(x) = [x[1]+x[2]; exp(-x[1]*x[2])]; # arbitrary function\ny = f.(x);\n\ndata = DataSet(x,y);\nprintln(data.x)\nprintln(data.y)","category":"page"},{"location":"structs/data/#scalings","page":"Data Sets","title":"Scaling Data Sets","text":"","category":"section"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"It is often helpful to scale the training data (setting means to zero and variances to unity).  These can be accomplished with the scalings structure and the associated functions.","category":"page"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"    get_scalings\n    scale_data!\n    rescale_data!","category":"page"},{"location":"structs/data/#ARFF.get_scalings","page":"Data Sets","title":"ARFF.get_scalings","text":"get_scalings(data)\n\nFind the means and variances of the data for scaling.\n\nFields\n\ndata - The training data set\n\n\n\n\n\nget_scalings(data)\n\nFind the means and variances of the data for scaling\n\nFields\n\ndata - The training data set\n\n\n\n\n\n","category":"function"},{"location":"structs/data/#ARFF.scale_data!","page":"Data Sets","title":"ARFF.scale_data!","text":"scale_data!(data, scalings)\n\nScale the data set (in-place) according to the specified scalings\n\nFields\n\ndata - Data set to be scale\nscalings - Scalings to apply to data\n\n\n\n\n\nscale_data!(data, scalings)\n\nScale the data set (in-place) according to the specified scalings\n\nFields\n\ndata - Data set to be scale\nscalings - Scalings to apply to data\n\n\n\n\n\n","category":"function"},{"location":"structs/data/#ARFF.rescale_data!","page":"Data Sets","title":"ARFF.rescale_data!","text":"rescale_data!(data, scalings)\n\nRescale the data set (in-place) according back to the original units\n\nFields\n\ndata - Data set to be scale\nscalings - Scalings to apply to data\n\n\n\n\n\nrescale_data!(data, scalings)\n\nRescale the data set (in-place) according back to the original units\n\nFields\n\ndata - Data set to be scale\nscalings - Scalings to apply to data\n\n\n\n\n\n","category":"function"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"For example, the following code obtains the means and variances in the data:","category":"page"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"scalings = get_scalings(data);\nprintln(scalings.μx);\nprintln(scalings.σ2x);\nprintln(scalings.μy);\nprintln(scalings.σ2y);","category":"page"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"Next, we can scale our data:","category":"page"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"scale_data!(data, scalings);\n\nusing Statistics\nprintln(mean(data.x));\nprintln(var(data.x));\nprintln(mean(data.y));\nprintln(var(data.y));","category":"page"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"If needed, we can then undo the scaling,","category":"page"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"rescale_data!(data, scalings)\n\nprintln(mean(data.x));\nprintln(var(data.x));\nprintln(mean(data.y));\nprintln(var(data.y));","category":"page"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"Note that these are in agreement with what was contained in our scalings structure.","category":"page"},{"location":"structs/data/#bias","page":"Data Sets","title":"Adding Bias","text":"","category":"section"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"It may be neccessary to modify an existing data set so as to include a constant bias term in a model.  If xin mathbbR^d, then ","category":"page"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"xmapsto (x1)=tildexin mathbbR^d+1","category":"page"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"This is relevant when using generalized activation functions; see Scalar Example with Generalized Activation Functions.  We provide tools for account for the constant in both the DataSet and DataScalings types:","category":"page"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"append_bias(data::ARFF.ScalarDataSet{TR,TY,TI}) where {TY<:Number,TR<:AbstractFloat,TI<:Integer}\nappend_bias(scalings::ARFF.ScalarDataScalings{TR,TY}) where {TY<:Number,TR<:AbstractFloat}","category":"page"},{"location":"structs/data/#ARFF.append_bias-Union{Tuple{ARFF.ScalarDataSet{TR, TY, TI}}, Tuple{TI}, Tuple{TR}, Tuple{TY}} where {TY<:Number, TR<:AbstractFloat, TI<:Integer}","page":"Data Sets","title":"ARFF.append_bias","text":"append_bias(data)\n\nCreate a new DataSet with a bias term appended to the end of the x coordinate.\n\nFields\n\ndata - data set to be augmented\n\n\n\n\n\n","category":"method"},{"location":"structs/data/#ARFF.append_bias-Union{Tuple{ARFF.ScalarDataScalings{TR, TY}}, Tuple{TR}, Tuple{TY}} where {TY<:Number, TR<:AbstractFloat}","page":"Data Sets","title":"ARFF.append_bias","text":"append_bias(scalings)\n\nCreate a new DataScalings accounting for a bias term appended to the end of the x coordinate.\n\nFields\n\nscalings - scalings to be augmented\n\n\n\n\n\n","category":"method"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"In this framework, we would compute and/or apply the scalings before adding the bias term to the data set, and then add the bias in to both the data and the scaling structures.  This avoids a potential divide by zero issue.","category":"page"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"using ARFF\nusing Statistics\nusing Printf","category":"page"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"n_x = 10\nx = [Float64[i] for i in 1:n_x]\nf(x) = x[1]^2\ny = f.(x)\ndata_ = DataSet(x, y)\nscalings_ = get_scalings(data_);\ndata_scaled = scale_data!(data_, scalings_);","category":"page"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"The means and variances of the scaled data set are as we would expect:","category":"page"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"println(mean(data_scaled.x));\nprintln(var(data_scaled.x));","category":"page"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"and the scalings_ has the relevant information:","category":"page"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"println(scalings_.μx);\nprintln(scalings_.σ2x);","category":"page"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"Next, we can add in the bias into the data set and check that it is computing properly:","category":"page"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"data_bias = append_bias(data_scaled);\nprintln(mean(data_bias.x));\nprintln(var(data_bias.x));","category":"page"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"and analogously with the scalings:","category":"page"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"scalings_bias = append_bias(scalings_);\nprintln(scalings_bias.μx);\nprintln(scalings_bias.σ2x);","category":"page"},{"location":"structs/fourier/#fourier","page":"Fourier Feature Models","title":"Fourier Feature Models","text":"","category":"section"},{"location":"structs/fourier/","page":"Fourier Feature Models","title":"Fourier Feature Models","text":"The generalized Fourier features model approximates functions with K features, with acvtivation function varphi as","category":"page"},{"location":"structs/fourier/","page":"Fourier Feature Models","title":"Fourier Feature Models","text":"f^dagger(x) approx f(x) = sum_k=1^K beta_k varphi(xomega_k)","category":"page"},{"location":"structs/fourier/","page":"Fourier Feature Models","title":"Fourier Feature Models","text":"In the above expression, xomega_kin mathbbR^d, while beta_kinmathbbR^d or beta_kinmathbbC^d, depending on the choice of activation function.  Consequently, our model is uniquely determined by the coefficients, the wavenumbers, and the choice of activiation functions; see [Activation Functions]","category":"page"},{"location":"structs/fourier/","page":"Fourier Feature Models","title":"Fourier Feature Models","text":"These are stored in either a scalar or vector valued data structure:","category":"page"},{"location":"structs/fourier/","page":"Fourier Feature Models","title":"Fourier Feature Models","text":"ARFF.ScalarFourierModel\nARFF.VectorFourierModel","category":"page"},{"location":"structs/fourier/#ARFF.ScalarFourierModel","page":"Fourier Feature Models","title":"ARFF.ScalarFourierModel","text":"ScalarFourierModel\n\nStructure containing a scalar valued fourier model which will be learned\n\nFields\n\nβ - Array of complex coefficients\nω - Array of wave vectors\nK - Number of Fourier features\ndx - Dimension of x coordinate\nϕ - Activation function\n\n\n\n\n\n","category":"type"},{"location":"structs/fourier/#ARFF.VectorFourierModel","page":"Fourier Feature Models","title":"ARFF.VectorFourierModel","text":"VectorFourierModel\n\nStructure containing a scalar valued fourier model which will be learned\n\nFields\n\nβ - 2D array of coefficients; K by dy in size\nω - Array of wave vectors\nK - Number of Fourier features\ndx - Dimension of x coordinate\ndy - Dimension of y coordinate\n\n\n\n\n\n","category":"type"},{"location":"structs/fourier/","page":"Fourier Feature Models","title":"Fourier Feature Models","text":"In the vector valued case, the beta_k's are stored in a matrix of size Ktimes d_y, while they are stored as a column vector in hte scalar case.","category":"page"},{"location":"structs/fourier/#Constructing-a-Fourier-Features-Model","page":"Fourier Feature Models","title":"Constructing a Fourier Features Model","text":"","category":"section"},{"location":"structs/fourier/","page":"Fourier Feature Models","title":"Fourier Feature Models","text":"A Fourier features model can be instantiated with:","category":"page"},{"location":"structs/fourier/","page":"Fourier Feature Models","title":"Fourier Feature Models","text":"ARFF.FourierModel(β::Vector{TY}, ω::Vector{Vector{TR}}) where {TY<:Number,TR<:AbstractFloat}\nARFF.FourierModel(β::Vector{TY}, ω::Vector{Vector{TR}}, ϕ::TA) where {TY<:Number,TR<:AbstractFloat,TA<:ActivationFunction{TY}}","category":"page"},{"location":"structs/fourier/#ARFF.FourierModel-Union{Tuple{TR}, Tuple{TY}, Tuple{Vector{TY}, Array{Vector{TR}, 1}}} where {TY<:Number, TR<:AbstractFloat}","page":"Fourier Feature Models","title":"ARFF.FourierModel","text":"FourierModel(β, ω)\n\nConstructor for a Fourier features model. Defaults to complex exponentials for activation functions.\n\nFields\n\nβ - Array of coefficients\nω - Array of wave numbers\n\n\n\n\n\n","category":"method"},{"location":"structs/fourier/#ARFF.FourierModel-Union{Tuple{TA}, Tuple{TR}, Tuple{TY}, Tuple{Vector{TY}, Array{Vector{TR}, 1}, TA}} where {TY<:Number, TR<:AbstractFloat, TA<:(ActivationFunction{TY})}","page":"Fourier Feature Models","title":"ARFF.FourierModel","text":"FourierModel(β, ω, ϕ)\n\nConstructor for a Fourier features model.\n\nFields\n\nβ - Array of coefficients\nω - Array of wave numbers\nϕ - Activation function of ActivationFunction type.  The data type of the β must agree with the data type of the range of ϕ.\n\n\n\n\n\n","category":"method"},{"location":"structs/fourier/","page":"Fourier Feature Models","title":"Fourier Feature Models","text":"For the vector valued case, β may be passed in as a (K, dy) sized matrix, or as a vector of length K containing vectors of size dy.  In this latter case, it is transformed over to the matrix layout.","category":"page"},{"location":"structs/fourier/","page":"Fourier Feature Models","title":"Fourier Feature Models","text":"As an example, the scalar valued Fourier model with complex exponentials can be instantiated as:","category":"page"},{"location":"structs/fourier/","page":"Fourier Feature Models","title":"Fourier Feature Models","text":"using ARFF\nusing Random\nRandom.seed!(200); # for reproducibility\n\nK = 10;\nd = 2;\n# convenience\nF = FourierModel(randn(ComplexF64, K),  [randn(d) for _ in 1:K]);","category":"page"},{"location":"structs/fourier/","page":"Fourier Feature Models","title":"Fourier Feature Models","text":"This defines F with random wavenumbers and amplitudes.  Strictly speaking, this is not required, but it is often helpful within the learning context that we will apply this method.  By not specifying an activation function, this defaults to FourierActivation function, e^i omega cdot x, and presumes all y related values are of the same complex type.","category":"page"},{"location":"structs/fourier/","page":"Fourier Feature Models","title":"Fourier Feature Models","text":"Function evaluation has been overloaded for a FourierModel, allowing us to evaluate it at points:","category":"page"},{"location":"structs/fourier/","page":"Fourier Feature Models","title":"Fourier Feature Models","text":"x_test = [0., 1.];\nprintln(F(x_test));","category":"page"},{"location":"structs/fourier/","page":"Fourier Feature Models","title":"Fourier Feature Models","text":"and it can perform a vectorized evaluation:","category":"page"},{"location":"structs/fourier/","page":"Fourier Feature Models","title":"Fourier Feature Models","text":"n = 10; # number of sample points\nx = [rand(2) for _ in 1:n];\nf(x) = exp(-x[1]*x[2]); # arbitrary function\ny = complex.(f.(x)); # make y valued data complex for type consistency\n\ndata = DataSet(x,y);\n\nF.(data.x);","category":"page"},{"location":"structs/fourier/","page":"Fourier Feature Models","title":"Fourier Feature Models","text":"As is sometimes the case, we may scale our data, train in the scale coordinate, and wish to evaluate new poitns in the original, unscaled, coordinate system. We can accomplish that by passing a DataScalings argument in for evaluation:","category":"page"},{"location":"structs/fourier/","page":"Fourier Feature Models","title":"Fourier Feature Models","text":"scalings = get_scalings(data);\n\nF(x_test, scalings)","category":"page"},{"location":"structs/fourier/","page":"Fourier Feature Models","title":"Fourier Feature Models","text":"If one wishes to use a different activation function, we pass that as an argument in the construction:","category":"page"},{"location":"structs/fourier/","page":"Fourier Feature Models","title":"Fourier Feature Models","text":"G = FourierModel(randn(K),  [randn(d) for _ in 1:K],SigmoidActivation);\nprintln(G([0.1, 0.2]))","category":"page"},{"location":"train/#Training","page":"Training","title":"Training","text":"","category":"section"},{"location":"train/","page":"Training","title":"Training","text":"Pages = [\"train.md\"]","category":"page"},{"location":"train/","page":"Training","title":"Training","text":"The key function is train_rwm!, which performs in place training on the model. This is implemented to handle the training data in several ways:","category":"page"},{"location":"train/","page":"Training","title":"Training","text":"A single DataSet can be provided and used in every training epoch.\nA single DataSet and a minibatch size can be provided, and minibatchs will be generated at each epoch.\nAn array of DataSet types can be provided, and they will be cycled through each epoch;","category":"page"},{"location":"train/#In-Place-Training","page":"Training","title":"In Place Training","text":"","category":"section"},{"location":"train/","page":"Training","title":"Training","text":"For both the scalar and vector valued case, we have the following commands: <!– ```@docs     trainrwm!(F::ARFF.ScalarFourierModel{TR,TB,TI,TA}, data::ARFF.ScalarDataSet{TR,TB,TI}, Σ::Matrix{TR}, options::ARFF.ARFFOptions; showprogress=true, recordloss=true) where {TB<:Number,TR<:AbstractFloat,TI<:Integer, TA<:ARFF.ActivationFunction{TB}}     trainrwm!(F::ARFF.ScalarFourierModel{TR,TB,TI,TA}, data::ARFF.ScalarDataSet{TR,TB,TI}, batchsize::TI, Σ::Matrix{TR}, options::ARFF.ARFFOptions; showprogress=true, recordloss=true) where {TB<:Number,TR<:AbstractFloat,TI<:Integer,TA<:ARFF.ActivationFunction{TB}}         trainrwm!(F::ARFF.ScalarFourierModel{TR,TB,TI,TA}, datasets::Vector{ARFF.ScalarDataSet{TR,TB,TI}}, Σ::Matrix{TR}, options::ARFF.ARFFOptions; showprogress=true, record_loss=true) where {TB<:Number,TR<:AbstractFloat,TI<:Integer,TA<:ARFF.ActivationFunction{TB}} ","category":"page"},{"location":"train/","page":"Training","title":"Training","text":"Having created an initial `F` we can then call","category":"page"},{"location":"train/","page":"Training","title":"Training","text":"Σmean, acceptancerate, loss = train_rwm!(F, data, Σ0, opts);","category":"page"},{"location":"train/","page":"Training","title":"Training","text":"The returned quantities are the mean adapted covariance matrix `Σ_mean`.  The\n`acceptance_rate` is the mean acceptance rate at each epoch, averaged overa the\ninternal steps,  `K * n_ω_steps`.  The loss is the recorded training loss,\nstored in `opts.loss`, at each epoch.\n\n## Recording the Training Trajectory\nWe have also included routines which record the model at each epoch.  These are called in the same way as above:","category":"page"},{"location":"train/","page":"Training","title":"Training","text":"Ftrajectory, Σmean, acceptancerate, loss = trainrwm(F0, data, Σ0, opts);","category":"page"},{"location":"train/","page":"Training","title":"Training","text":"\nThe functions are analogously named:","category":"page"},{"location":"train/","page":"Training","title":"Training","text":"@docs     trainrwm(F::ARFF.ScalarFourierModel{TR,TB,TI,TA}, data::ARFF.ScalarDataSet{TR,TB,TI}, Σ::Matrix{TR}, options::ARFF.ARFFOptions; showprogress=true, recordloss=true) where {TB<:Number,TR<:AbstractFloat,TI<:Integer, TA<:ARFF.ActivationFunction{TB}}     trainrwm(F::ARFF.ScalarFourierModel{TR,TB,TI,TA}, data::ARFF.ScalarDataSet{TR,TB,TI}, batchsize::TI, Σ::Matrix{TR}, options::ARFF.ARFFOptions; showprogress=true, recordloss=true) where {TB<:Number,TR<:AbstractFloat,TI<:Integer,TA<:ARFF.ActivationFunction{TB}}         trainrwm(F::ARFF.ScalarFourierModel{TR,TB,TI,TA}, datasets::Vector{ARFF.ScalarDataSet{TR,TB,TI}}, Σ::Matrix{TR}, options::ARFF.ARFFOptions; showprogress=true, record_loss=true) where {TB<:Number,TR<:AbstractFloat,TI<:Integer,TA<:ARFF.ActivationFunction{TB}} ``` This records the result at the end of each epoch.","category":"page"},{"location":"resample/resample/#Resampling","page":"Resampling","title":"Resampling","text":"","category":"section"},{"location":"resample/resample/","page":"Resampling","title":"Resampling","text":"It has been shown in [2] that the performance of the training algorithm can be significantly improved if a resampling step is done at each epoch. This resampling is done by generating a Probability Mass Function hatp = (hatp_1   hatp_K) with values propto beta_k:","category":"page"},{"location":"resample/resample/","page":"Resampling","title":"Resampling","text":"hatp_k = fracbeta_ksum_jbeta_j","category":"page"},{"location":"resample/resample/","page":"Resampling","title":"Resampling","text":"This is then used to estimate the Effective Sample Size (ESS):","category":"page"},{"location":"resample/resample/","page":"Resampling","title":"Resampling","text":"mathrmESS= leftsum_k hatp_k^2right^-1","category":"page"},{"location":"resample/resample/","page":"Resampling","title":"Resampling","text":"We then compare the ESS to Rcdot K, where Rin 01, is a tolerance parameter.  When R = 1, the resampling will be done on each epoch of the training process and when R = 0, there will be no resampling; this is equivalent to calling trivial_resample!. R can be tuned for individual problems.","category":"page"},{"location":"resample/resample/","page":"Resampling","title":"Resampling","text":"When the ESS falls below Rcdot K, we proceed as follows.  We resample K independent index values i_1  i_K from 1 2  K using the PMF hatp. Using These resampled indicies then determine the new wavenumbers boldsymbolomega = omega_i_1  omega_i_K, and we solve the linear system (again) to obtain the coefficient vector hatboldsymbolbeta = (hatbeta_1  hatbeta_K).    ","category":"page"},{"location":"resample/resample/","page":"Resampling","title":"Resampling","text":"The resampler can be called in the following way:","category":"page"},{"location":"resample/resample/","page":"Resampling","title":"Resampling","text":"resample!","category":"page"},{"location":"resample/resample/#ARFF.resample!","page":"Resampling","title":"ARFF.resample!","text":"resample!(F, x, y, S, epoch, linear_solver!; R=1.0)\n\nFunction that performs the resampling step of the training algorithm.\n\nFields\n\nF - A FourierModel structure\nx - the x coordinates in training data\ny - the y coordinates in training data\nS - the design matrix\nepoch - the current epoch of the training process\nlinear_solver! - an in place solver for the linear system\nR = 1.0 - Effective sample size threshold\n\n\n\n\n\n","category":"function"},{"location":"resample/resample/","page":"Resampling","title":"Resampling","text":"The linear solver you provide msut be compatible with ARFF.jl, and must accept the following arugments linear_solver!(β, ω, x, y, S, epoch).","category":"page"},{"location":"examples/example3/#Example-with-Generalized-Activation-Functions-and-Scalings","page":"Example with Generalized Activation Functions and Scalings","title":"Example with Generalized Activation Functions and Scalings","text":"","category":"section"},{"location":"examples/example3/","page":"Example with Generalized Activation Functions and Scalings","title":"Example with Generalized Activation Functions and Scalings","text":"This example demonstates learning with generalized activation functions, a bias term, and scalings.  For this problem, ","category":"page"},{"location":"examples/example3/","page":"Example with Generalized Activation Functions and Scalings","title":"Example with Generalized Activation Functions and Scalings","text":"f(x) = expleft(-frac12left(x_1^2 + tfrac1100x_2^2 right)right)","category":"page"},{"location":"examples/example3/","page":"Example with Generalized Activation Functions and Scalings","title":"Example with Generalized Activation Functions and Scalings","text":"As was the case in Scalar Example with Generalized Activation Functions, we will make use of sigmoid activations and augment the problem with a bias. What is a bit different here is that we will also scale the data.","category":"page"},{"location":"examples/example3/#Generate-Training-Data","page":"Example with Generalized Activation Functions and Scalings","title":"Generate Training Data","text":"","category":"section"},{"location":"examples/example3/","page":"Example with Generalized Activation Functions and Scalings","title":"Example with Generalized Activation Functions and Scalings","text":"using Random\nusing Distributions\nusing Plots\nusing LinearAlgebra\nusing ARFF","category":"page"},{"location":"examples/example3/","page":"Example with Generalized Activation Functions and Scalings","title":"Example with Generalized Activation Functions and Scalings","text":"First, we will generate and visualize the training data:","category":"page"},{"location":"examples/example3/","page":"Example with Generalized Activation Functions and Scalings","title":"Example with Generalized Activation Functions and Scalings","text":"f(x) = exp(-0.5 * (x[1]^2+ x[2]^2/100));\n\nn_x = 1000; # number of training points\n\nRandom.seed!(100); # for reproducibility\n# generate n_x sample points\nd_ = 2;\nx = [[rand(Uniform(-5.0, 5.0)), rand(Uniform(-50.0, 50.0))] for _ in 1:n_x];\ny = f.(x);\n\n# store data in DataSet structure\ndata_ = DataSet(x,y);\n# scale the data\nscalings_ = get_scalings(data_);\nscale_data!(data_, scalings_)\n# append the bias term to our data and scalings\ndata = append_bias(data_);\nscalings = append_bias(scalings_);\nd  = d_ + 1; nothing","category":"page"},{"location":"examples/example3/#Initialize-Fourier-Model-with-Generalized-Features","page":"Example with Generalized Activation Functions and Scalings","title":"Initialize Fourier Model with Generalized Features","text":"","category":"section"},{"location":"examples/example3/","page":"Example with Generalized Activation Functions and Scalings","title":"Example with Generalized Activation Functions and Scalings","text":"Next, we need to initialize our FourierModel","category":"page"},{"location":"examples/example3/","page":"Example with Generalized Activation Functions and Scalings","title":"Example with Generalized Activation Functions and Scalings","text":"K = 2^7;\nRandom.seed!(200); # for reproducibility\nF0 = FourierModel([1. *randn() for _ in 1:K],  \n    [randn(d) for _ in 1:K],SigmoidActivation); nothing","category":"page"},{"location":"examples/example3/#Set-Parameters-and-Train","page":"Example with Generalized Activation Functions and Scalings","title":"Set Parameters and Train","text":"","category":"section"},{"location":"examples/example3/","page":"Example with Generalized Activation Functions and Scalings","title":"Example with Generalized Activation Functions and Scalings","text":"δ = 0.1; # rwm step size\nλ = 1e-6; # regularization\nn_epochs = 10^3; # total number of iterations\nn_rwm_steps = 10; # number of steps between full β updates\nn_burn = n_epochs ÷ 10;\n\nlinear_solver! = (β, ω, x, y, S, epoch)-> solve_normal!(β, S, y, λ=λ);\n\nrwm_sampler = AdaptiveRWMSampler(F0, linear_solver!, n_rwm_steps, n_burn, δ);\n\nRandom.seed!(1000); # for reproducibility\nF = deepcopy(F0);\nacceptance_rate, loss= train_rwm!(F, data, rwm_sampler, n_epochs, show_progress=false); nothing ","category":"page"},{"location":"examples/example3/#Evaluate-Results","page":"Example with Generalized Activation Functions and Scalings","title":"Evaluate Results","text":"","category":"section"},{"location":"examples/example3/","page":"Example with Generalized Activation Functions and Scalings","title":"Example with Generalized Activation Functions and Scalings","text":"Looking at the trianing loss, we see the model appears to be well trained for the selected width, K:","category":"page"},{"location":"examples/example3/","page":"Example with Generalized Activation Functions and Scalings","title":"Example with Generalized Activation Functions and Scalings","text":"plot(1:length(loss), loss, yscale=:log10, label=\"\")\nxlabel!(\"Epoch\")\nylabel!(\"Loss\")","category":"page"},{"location":"examples/example3/","page":"Example with Generalized Activation Functions and Scalings","title":"Example with Generalized Activation Functions and Scalings","text":"We verify that the training data is well fit:","category":"page"},{"location":"examples/example3/","page":"Example with Generalized Activation Functions and Scalings","title":"Example with Generalized Activation Functions and Scalings","text":"scatter(data.y,F.(data.x),label=\"Data\")\nxx = LinRange(minimum(data.y),maximum(data.y),100);\nplot!(xx, xx, ls=:dash, label=\"\")\nxlabel!(\"Truth\")\nylabel!(\"Prediction\")","category":"page"},{"location":"examples/example3/","page":"Example with Generalized Activation Functions and Scalings","title":"Example with Generalized Activation Functions and Scalings","text":"Lastly, a direct comparison of the learned approximation of f:","category":"page"},{"location":"examples/example3/","page":"Example with Generalized Activation Functions and Scalings","title":"Example with Generalized Activation Functions and Scalings","text":"xx = LinRange(-2, 2, 100)\nyy = LinRange(-20, 20, 100)\n\np1 = contourf(xx, yy, [f([x_, y_]) for y_ in yy, x_ in xx],levels=LinRange(-0.1,1.1,20), colorbar=:false)\nxlabel!(p1, \"x\")\nylabel!(p1, \"y\")\ntitle!(p1, \"Truth\")\np2 = contourf(xx, yy, [F([x_, y_, 1], scalings) for y_ in yy, x_ in xx], levels=LinRange(-0.1, 1.1, 20), colorbar=:false)\nxlabel!(p2, \"x\")\ntitle!(p2, \"Learned Model\")\nplot(p1, p2, layout=(1, 2))","category":"page"},{"location":"aux/#Auxiliary-Functions-and-Utilities","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"","category":"section"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"Pages = [\"aux.md\"]","category":"page"},{"location":"aux/#linalg","page":"Auxiliary Functions and Utilities","title":"Linear Algebra","text":"","category":"section"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"It is essential to be able solve for the updated boldsymbolbeta when we update the boldsymbolomega.  In a typical setting, this corresponds to solving","category":"page"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"(S^astS + N lambda I)boldsymbolbeta = S^ast boldsymboly","category":"page"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"We have included two naive solvers for this problem:","category":"page"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"solve_normal!\nsolve_normal_svd!","category":"page"},{"location":"aux/#ARFF.solve_normal!","page":"Auxiliary Functions and Utilities","title":"ARFF.solve_normal!","text":"solve_normal!(β, S, y_data; λ = 1e-8)\n\nSolve the regularized linear system using the normal equations.\n\nFields\n\nβ - The vector of coefficients that will be obtained\nS - The design matrix\ny_data - y coordinates\nλ = 1e-8 - Regularization parameter\n\n\n\n\n\n","category":"function"},{"location":"aux/#ARFF.solve_normal_svd!","page":"Auxiliary Functions and Utilities","title":"ARFF.solve_normal_svd!","text":"solve_normal_svd!(β, S, y_data; λ = 1e-8)\n\nSolve the regularized linear system using the SVD.\n\nFields\n\nβ - The vector of coefficients that will be obtained\nS - The design matrix\ny_data - y coordinates\nλ = 1e-8 - Regularization parameter\n\n\n\n\n\n","category":"function"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"Other formulations may be more appropriate.  Indeed, in [3], the authors use the regularized loss function in the spirit of Sobolev:","category":"page"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"Sboldsymbolbeta-boldsymboly_2^2 + lambda sum_k (1+omega_k^2)beta_k^2","category":"page"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"When using the linear solvers as part of an ARFFSolver or AdaptiveRWMSampler, the expectation is that they can be called as linear_solve!(β, ω, x, y, S, epoch).  Thus, you will see, in examples, things implemented as:","category":"page"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"linear_solver! = (β, ω, x, y, S, epoch)-> solve_normal!(β, S, y);","category":"page"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"and linear_solver! is then used in the rest of the code.  ","category":"page"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"In the case that one is solving a vector valued problem, the vector valued beta's are obtained component by component against the vector valued y's. The following bit of code will perform the component by component solve in a way that is consistent with the rest of ARFF.jl,","category":"page"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"β_ = similar(F0.β[:, 1]); # allocate workspace memory\nfunction component_solver!(β, ω, x, y, S, epoch)\n    for d_ in 1:dy\n        solve_normal!(β_, S, @view(y[:, d_]))\n        @. β[:, d_] = β_\n    end\n    β\nend","category":"page"},{"location":"aux/#loss","page":"Auxiliary Functions and Utilities","title":"Loss Functions","text":"","category":"section"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"ARFF.mse_loss(F::ARFF.ScalarFourierModel{TR,TY,TI,TA}, data_x::AbstractVector{Vector{TR}}, data_y::AbstractVector{TY}) where {TR<:AbstractFloat,TY<:Number,TI<:Integer,TA<:ARFF.ActivationFunction{TY}}","category":"page"},{"location":"aux/#ARFF.mse_loss-Union{Tuple{TA}, Tuple{TI}, Tuple{TY}, Tuple{TR}, Tuple{ARFF.ScalarFourierModel{TR, TY, TI, TA}, AbstractArray{Vector{TR}, 1}, AbstractVector{TY}}} where {TR<:AbstractFloat, TY<:Number, TI<:Integer, TA<:(ActivationFunction{TY})}","page":"Auxiliary Functions and Utilities","title":"ARFF.mse_loss","text":"mse_loss(F, data_x, data_y)\n\nMean squared error loss function\n\nFields\n\nF - A FourierModel structure\ndata_x - the x coordinates in training data\ndata_y - the y coordinates in training data\n\n\n\n\n\n","category":"method"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"As the entire framework is built around the mean square loss function, we have included it for convenience.  Other loss functions can be implemented, but they should have the calling sequence:","category":"page"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"function loss_function(F, data_x, data_y)\n    # compute loss \n    return loss\nend","category":"page"},{"location":"aux/#Other-Utilities","page":"Auxiliary Functions and Utilities","title":"Other Utilities","text":"","category":"section"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"optimal_γ","category":"page"},{"location":"aux/#ARFF.optimal_γ","page":"Auxiliary Functions and Utilities","title":"ARFF.optimal_γ","text":"optimal_γ(d)\n\nCompute the optimal γ parameter as a function of dimension d\n\nFields\n\nd - the dimension of the x coordinate\n\n\n\n\n\n","category":"function"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"Following Remark 1 in [1], the optimal gamma corresponds to gamma = 3d -2, which is encoded in the above function.","category":"page"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"trivial_resample!\ntrivial_mutate!","category":"page"},{"location":"aux/#ARFF.trivial_resample!","page":"Auxiliary Functions and Utilities","title":"ARFF.trivial_resample!","text":"trivial_resample!(F, x, y, S, epoch)\n\nTrivial resampling step.  Used for training strategies which only rely upon mutation.\n\nFields\n\nF - A FourierModel structure\nx - vector of x coordinates of training data\ny - vector of y coodriantes of training data\nS - design matrix\nepoch - training epoch\n\n\n\n\n\n","category":"function"},{"location":"aux/#ARFF.trivial_mutate!","page":"Auxiliary Functions and Utilities","title":"ARFF.trivial_mutate!","text":"trivial_mutate!(F, x, y, S, epoch)\n\nTrivial mutation step.  Generally used onmly for testing.\n\nFields\n\nF - A FourierModel structure\nx - vector of x coordinates of training data\ny - vector of y coodriantes of training data\nS - design matrix\nepoch - training epoch\n\n\n\n\n\n","category":"function"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"These functions can be used within an ARFFSolver data structure.","category":"page"},{"location":"examples/example1/#Scalar-Example","page":"Scalar Example","title":"Scalar Example","text":"","category":"section"},{"location":"examples/example1/","page":"Scalar Example","title":"Scalar Example","text":"This example is taken from Section 5 of [1]. Consider trying to learn the scalar mapping","category":"page"},{"location":"examples/example1/","page":"Scalar Example","title":"Scalar Example","text":"f(x) = mathrmSileft(fracxaright)e^-fracx^22 quad a0","category":"page"},{"location":"examples/example1/","page":"Scalar Example","title":"Scalar Example","text":"where mathrmSi is the Sine integral.  To make the problem somewhat challenging, we take a=10^-3.","category":"page"},{"location":"examples/example1/#Generate-Training-Data","page":"Scalar Example","title":"Generate Training Data","text":"","category":"section"},{"location":"examples/example1/","page":"Scalar Example","title":"Scalar Example","text":"First, we will generate and visualize the training data:","category":"page"},{"location":"examples/example1/","page":"Scalar Example","title":"Scalar Example","text":"using SpecialFunctions\nusing Random\nusing Plots\nusing LinearAlgebra\nusing ARFF\n\na = 1e-3;\nf(x) = sinint(x/a) * exp(-0.5 * (x^2));\n\nn_x = 500; # number of training points\nd = 1;\nRandom.seed!(100); # for reproducibility\nx = [0.1*rand(d) for _ in 1:n_x];\ny = [f(x_[1]) for x_ in x];\n\n# store data in DataSet structure\ndata = DataSet(x,complex.(y));\n\nscatter([x_[1] for x_ in x], y, label=\"Sample Points\")\nxx = LinRange(0, 0.1, 500);\nplot!(xx, f.(xx), label=\"f(x)\")\nxlabel!(\"x\")","category":"page"},{"location":"examples/example1/","page":"Scalar Example","title":"Scalar Example","text":"Note: When the domain of our target function is mathbbR^1, the x-data must still be stored as an array of arrays of length one, not an array of scalars.  We also do not require DataScalings for this problem.","category":"page"},{"location":"examples/example1/#Initialize-Fourier-Model","page":"Scalar Example","title":"Initialize Fourier Model","text":"","category":"section"},{"location":"examples/example1/","page":"Scalar Example","title":"Scalar Example","text":"Next, we need to initialize our FourierModel","category":"page"},{"location":"examples/example1/","page":"Scalar Example","title":"Scalar Example","text":"K = 2^7;\nRandom.seed!(200); # for reproducibility\nF0 = FourierModel([1. *randn(ComplexF64) for _ in 1:K],  \n    [randn(d) for _ in 1:K]); nothing","category":"page"},{"location":"examples/example1/","page":"Scalar Example","title":"Scalar Example","text":"This defaults to complex exponential activation functions.","category":"page"},{"location":"examples/example1/#Set-Parameters-and-Train","page":"Scalar Example","title":"Set Parameters and Train","text":"","category":"section"},{"location":"examples/example1/","page":"Scalar Example","title":"Scalar Example","text":"δ = 10.; # rwm step size\nλ = 1e-6; # regularization\nn_epochs = 10^3; # number of epochs\nn_rwm_steps = 10; # number of RWM steps during mutation\nn_burn = n_epochs ÷ 10; # use 10% of the run for burn in\n\nlinear_solver! = (β, ω, x, y, S, epoch)-> solve_normal!(β, S, y, λ=λ);\n\nrwm_sampler = AdaptiveRWMSampler(F0, linear_solver!, n_rwm_steps, n_burn, δ);\n\nRandom.seed!(1000); # for reproducibility\nF = deepcopy(F0);\nacceptance_rate, loss= train_rwm!(F, data, rwm_sampler, n_epochs, show_progress=false); nothing ","category":"page"},{"location":"examples/example1/#Evaluate-Results","page":"Scalar Example","title":"Evaluate Results","text":"","category":"section"},{"location":"examples/example1/","page":"Scalar Example","title":"Scalar Example","text":"Looking at the trianing loss, we see the model appears to be well trained for the selected width, K:","category":"page"},{"location":"examples/example1/","page":"Scalar Example","title":"Scalar Example","text":"plot(1:length(loss), loss, yscale=:log10, label=\"\")\nxlabel!(\"Epoch\")\nylabel!(\"Loss\")","category":"page"},{"location":"examples/example1/","page":"Scalar Example","title":"Scalar Example","text":"Next, we can verify that we have a high quality approximation of the true f(x):","category":"page"},{"location":"examples/example1/","page":"Scalar Example","title":"Scalar Example","text":"xx = LinRange(0, .1, 500);\nscatter([x_[1] for x_ in x], y, label=\"Sample Points\", legend=:right)\nplot!(xx, f.(xx), label = \"Truth\" )\nplot!(xx, real.([F([x_]) for x_ in xx]),label=\"Learned Model (Real Part)\")\nplot!(xx, imag.([F([x_]) for x_ in xx]),label=\"Learned Model (Imaginary Part)\" )\nxlabel!(\"x\")","category":"page"},{"location":"examples/example1/","page":"Scalar Example","title":"Scalar Example","text":"We can also verify that the training data is well fit:","category":"page"},{"location":"examples/example1/","page":"Scalar Example","title":"Scalar Example","text":"scatter(real.(data.y),real.(F.(data.x)),label=\"Training Data\")\nxx = LinRange(0,2,100);\nplot!(xx, xx, ls=:dash, label=\"\")\nxlabel!(\"Truth\")\nylabel!(\"Prediction\")","category":"page"},{"location":"examples/example2/#Scalar-Example-with-Generalized-Activation-Functions","page":"Scalar Example with Generalized Activation Functions","title":"Scalar Example with Generalized Activation Functions","text":"","category":"section"},{"location":"examples/example2/","page":"Scalar Example with Generalized Activation Functions","title":"Scalar Example with Generalized Activation Functions","text":"This example demonstates learning with generalized activation functions.  For this problem, ","category":"page"},{"location":"examples/example2/","page":"Scalar Example with Generalized Activation Functions","title":"Scalar Example with Generalized Activation Functions","text":"f(x) = e^-x^22","category":"page"},{"location":"examples/example2/","page":"Scalar Example with Generalized Activation Functions","title":"Scalar Example with Generalized Activation Functions","text":"and we will work with sigmoid activation functions.  For this problem, we will augment our data to allow for a bias term, training, with an abuse of notation,","category":"page"},{"location":"examples/example2/","page":"Scalar Example with Generalized Activation Functions","title":"Scalar Example with Generalized Activation Functions","text":"f(x) approx F(tildex=(x1)) = sum_k beta_k varphi(tildexomega_k)","category":"page"},{"location":"examples/example2/","page":"Scalar Example with Generalized Activation Functions","title":"Scalar Example with Generalized Activation Functions","text":"The tildex is the variable padded with a unit value.  Thus, the problem will be studied as though it were over mathbbR^2.","category":"page"},{"location":"examples/example2/#Generate-Training-Data","page":"Scalar Example with Generalized Activation Functions","title":"Generate Training Data","text":"","category":"section"},{"location":"examples/example2/","page":"Scalar Example with Generalized Activation Functions","title":"Scalar Example with Generalized Activation Functions","text":"using Random\nusing Plots\nusing LinearAlgebra\nusing ARFF","category":"page"},{"location":"examples/example2/","page":"Scalar Example with Generalized Activation Functions","title":"Scalar Example with Generalized Activation Functions","text":"First, we will generate and visualize the training data:","category":"page"},{"location":"examples/example2/","page":"Scalar Example with Generalized Activation Functions","title":"Scalar Example with Generalized Activation Functions","text":"f(x) = exp(-0.5 * (x^2));\n\nn_x = 100; # number of training points\nd = 2;\nRandom.seed!(100); # for reproducibility\n# generate n_x sample points\nx = [[4*rand()] for _ in 1:n_x]; \ny = [f(x_[1]) for x_ in x];\n\n# store data in DataSet structure\ndata_ = DataSet(x,y);\n# append the bias term to our data\ndata = append_bias(data_);\n\nscatter([x_[1] for x_ in x], y, label=\"Sample Points\")\nxx = LinRange(0, 4, 100);\nplot!(xx, f.(xx), label=\"f(x)\")\nxlabel!(\"x\")","category":"page"},{"location":"examples/example2/#Initialize-Fourier-Model-with-Generalized-Features","page":"Scalar Example with Generalized Activation Functions","title":"Initialize Fourier Model with Generalized Features","text":"","category":"section"},{"location":"examples/example2/","page":"Scalar Example with Generalized Activation Functions","title":"Scalar Example with Generalized Activation Functions","text":"Next, we need to initialize our FourierModel","category":"page"},{"location":"examples/example2/","page":"Scalar Example with Generalized Activation Functions","title":"Scalar Example with Generalized Activation Functions","text":"K = 2^7;\nRandom.seed!(200); # for reproducibility\nF0 = FourierModel([1. *randn() for _ in 1:K],  \n    [randn(d) for _ in 1:K],SigmoidActivation); nothing","category":"page"},{"location":"examples/example2/#Set-Parameters-and-Train","page":"Scalar Example with Generalized Activation Functions","title":"Set Parameters and Train","text":"","category":"section"},{"location":"examples/example2/","page":"Scalar Example with Generalized Activation Functions","title":"Scalar Example with Generalized Activation Functions","text":"δ = 0.1; # rwm step size\nλ = 1e-6; # regularization\nn_epochs = 10^3; # total number of iterations\nn_rwm_steps = 10; # number of steps between full β updates\nn_burn = n_epochs ÷ 10;\n\nlinear_solver! = (β, ω, x, y, S, epoch)-> solve_normal!(β, S, y, λ=λ);\n\nrwm_sampler = AdaptiveRWMSampler(F0, linear_solver!, n_rwm_steps, n_burn, δ);\n\nRandom.seed!(1000); # for reproducibility\nF = deepcopy(F0);\nacceptance_rate, loss= train_rwm!(F, data, rwm_sampler,n_epochs,  show_progress=false); nothing ","category":"page"},{"location":"examples/example2/#Evaluate-Results","page":"Scalar Example with Generalized Activation Functions","title":"Evaluate Results","text":"","category":"section"},{"location":"examples/example2/","page":"Scalar Example with Generalized Activation Functions","title":"Scalar Example with Generalized Activation Functions","text":"Looking at the trianing loss, we see the model appears to be well trained for the selected width, K:","category":"page"},{"location":"examples/example2/","page":"Scalar Example with Generalized Activation Functions","title":"Scalar Example with Generalized Activation Functions","text":"plot(1:length(loss), loss, yscale=:log10, label=\"\")\nxlabel!(\"Epoch\")\nylabel!(\"Loss\")","category":"page"},{"location":"examples/example2/","page":"Scalar Example with Generalized Activation Functions","title":"Scalar Example with Generalized Activation Functions","text":"Next, we can verify that we have a high quality approximation of the true f(x):","category":"page"},{"location":"examples/example2/","page":"Scalar Example with Generalized Activation Functions","title":"Scalar Example with Generalized Activation Functions","text":"xx = LinRange(0, 4, 500);\nscatter([x_[1] for x_ in x], y, label=\"Sample Points\", legend=:right)\nplot!(xx, f.(xx), label = \"Truth\" )\nplot!(xx, [F([x_, 1]) for x_ in xx],label=\"Learned Model\")\nxlabel!(\"x\")","category":"page"},{"location":"examples/example2/","page":"Scalar Example with Generalized Activation Functions","title":"Scalar Example with Generalized Activation Functions","text":"We can also verify that the training data is well fit:","category":"page"},{"location":"examples/example2/","page":"Scalar Example with Generalized Activation Functions","title":"Scalar Example with Generalized Activation Functions","text":"scatter(data.y,F.(data.x),label=\"Data\")\nxx = LinRange(0,1,100);\nplot!(xx, xx, ls=:dash, label=\"\")\nxlabel!(\"Truth\")\nylabel!(\"Prediction\")","category":"page"},{"location":"examples/vector1/#Vector-Valued-Example","page":"Vector Valued Example","title":"Vector Valued Example","text":"","category":"section"},{"location":"examples/vector1/","page":"Vector Valued Example","title":"Vector Valued Example","text":"This example demonstarates learning a vector valued problem,","category":"page"},{"location":"examples/vector1/","page":"Vector Valued Example","title":"Vector Valued Example","text":"f(x) = beginpmatrixx_1 x_2x_1^2 - x_2^2endpmatrix","category":"page"},{"location":"examples/vector1/","page":"Vector Valued Example","title":"Vector Valued Example","text":"using Random\nusing Plots\nusing LinearAlgebra\nusing ARFF","category":"page"},{"location":"examples/vector1/","page":"Vector Valued Example","title":"Vector Valued Example","text":"# define components of vector valued function\nf1(x) = x[1]*x[2];\nf2(x) = x[1]^2 - x[2]^2;\n\nxx = LinRange(-2, 2, 101);\nyy = LinRange(-2, 2, 101);\n\nz1 = [f1([x_, y_]) for y_ in yy, x_ in xx];\nz2 = [f2([x_, y_]) for y_ in yy, x_ in xx];\n\np1 = contourf(xx, yy, z1)\nxlabel!(\"x\")\nylabel!(\"y\")\np2 = contourf(xx, yy, z2)\nxlabel!(\"x\")\nplot(p1, p2, layout=(1, 2), plot_title=\"Truth\")","category":"page"},{"location":"examples/vector1/#Generate-Training-Data","page":"Vector Valued Example","title":"Generate Training Data","text":"","category":"section"},{"location":"examples/vector1/","page":"Vector Valued Example","title":"Vector Valued Example","text":"First, we generate our training data","category":"page"},{"location":"examples/vector1/","page":"Vector Valued Example","title":"Vector Valued Example","text":"N = 10^3;\nd=2;\nRandom.seed!(100)\nx_data = [randn(2) for _ in 1:N];\ny_data = [[f1(x_), f2(x_)] for x_ in x_data];\ndata = DataSet(x_data, complex.(y_data)); nothing","category":"page"},{"location":"examples/vector1/#Initialize-Fourier-Model","page":"Vector Valued Example","title":"Initialize Fourier Model","text":"","category":"section"},{"location":"examples/vector1/","page":"Vector Valued Example","title":"Vector Valued Example","text":"Next, we need to initialize our FourierModel","category":"page"},{"location":"examples/vector1/","page":"Vector Valued Example","title":"Vector Valued Example","text":"K = 2^6;\nRandom.seed!(200)\nd = 2;\nF0 = FourierModel([1.0 * randn(d) for _ in 1:K], \n    [1.0 * randn(d) for _ in 1:K]); nothing","category":"page"},{"location":"examples/vector1/#Set-Parameters-and-Train","page":"Vector Valued Example","title":"Set Parameters and Train","text":"","category":"section"},{"location":"examples/vector1/","page":"Vector Valued Example","title":"Vector Valued Example","text":"δ = 0.1; # rwm step size\nλ = 1e-6; # regularization\nn_epochs = 10^3; # total number of iterations\nn_rwm_steps = 10; # number of steps between full β updates\nn_burn = n_epochs ÷ 10;\n\nβ_ = similar(F0.β[:, 1])\nfunction component_solver!(β, ω, x, y, S, epoch)\n    for d_ in 1:d\n        solve_normal!(β_, S, @view(y[:, d_]))\n        @. β[:, d_] = β_\n    end\n    β\nend\n\nrwm_sampler = AdaptiveRWMSampler(F0, component_solver!, n_rwm_steps, n_burn, δ)\n\nRandom.seed!(1000);\nF = deepcopy(F0);\nacceptance_rate, loss = train_rwm!(F, data, rwm_sampler,n_epochs, show_progress=false); nothing","category":"page"},{"location":"examples/vector1/#Evaluate-Results","page":"Vector Valued Example","title":"Evaluate Results","text":"","category":"section"},{"location":"examples/vector1/","page":"Vector Valued Example","title":"Vector Valued Example","text":"Looking at the trianing loss, we see the model appears to be well trained for the selected width, K:","category":"page"},{"location":"examples/vector1/","page":"Vector Valued Example","title":"Vector Valued Example","text":"plot(1:length(loss), loss, yscale=:log10, label=\"\")\nxlabel!(\"Epoch\")\nylabel!(\"Loss\")","category":"page"},{"location":"examples/vector1/","page":"Vector Valued Example","title":"Vector Valued Example","text":"Next, we look at how the components of the trained model look:","category":"page"},{"location":"examples/vector1/","page":"Vector Valued Example","title":"Vector Valued Example","text":"z1 = [real(F([x_, y_])[1]) for y_ in yy, x_ in xx];\nz2 = [real(F([x_, y_])[2]) for y_ in yy, x_ in xx];\n\np1 = contourf(xx, yy, z1)\nxlabel!(\"x\")\nylabel!(\"y\")\np2 = contourf(xx, yy, z2)\nxlabel!(\"x\")\nplot(p1, p2, layout=(1, 2),plot_title=\"Trained Model\")","category":"page"},{"location":"structs/activation/#activation","page":"Activation Functions","title":"Activation Functions","text":"","category":"section"},{"location":"structs/activation/","page":"Activation Functions","title":"Activation Functions","text":"While the theory has been developed for the Fourier activation functions of type e^i omega cdot x, this module permits for general activation funcitons to be encoded in an ActivationFunction data structure.  ","category":"page"},{"location":"structs/activation/","page":"Activation Functions","title":"Activation Functions","text":"ActivationFunction","category":"page"},{"location":"structs/activation/#ARFF.ActivationFunction","page":"Activation Functions","title":"ARFF.ActivationFunction","text":"ActivationFunction{TB,TF} <: AbstractActivationFunction where {TB<:Number,TF<:Function}\n\nActivation function data structure.  TB should match the desired type of y from the data.\n\nFields\n\nσ - A scalar valued function that returns the TB data type.\n\n\n\n\n\n","category":"type"},{"location":"structs/activation/","page":"Activation Functions","title":"Activation Functions","text":"This structure encodes both the function, along with the return data type (real or complex) to ensure type consistency for performance.  The following example would define the ReLU activation function:","category":"page"},{"location":"structs/activation/","page":"Activation Functions","title":"Activation Functions","text":"using ARFF\nrelu(z) = z*(z>0);\nReLUActivation = ActivationFunction{Float64}(relu)","category":"page"},{"location":"structs/activation/","page":"Activation Functions","title":"Activation Functions","text":"This can then be passed in as an argument when defining a Fourier feature model.","category":"page"},{"location":"structs/activation/","page":"Activation Functions","title":"Activation Functions","text":"The ActivationFunction type can be evaluated:","category":"page"},{"location":"structs/activation/","page":"Activation Functions","title":"Activation Functions","text":"println(ReLUActivation([-1.], [1.]));\nprintln(ReLUActivation([1.], [1.]));","category":"page"},{"location":"structs/activation/","page":"Activation Functions","title":"Activation Functions","text":"NOTE It is essential that the data type, TN, used when defining ActivationFunction{TN}(myactfunc) be the same as the numerical type of y used in the construction of a data set.","category":"page"},{"location":"structs/activation/","page":"Activation Functions","title":"Activation Functions","text":"Some convenience functions are a part of the package:","category":"page"},{"location":"structs/activation/","page":"Activation Functions","title":"Activation Functions","text":"FourierActivation - The complex exponential for type ComplexF64\nSigmoidActivation - The sigmoid function for type Float64\nArcTanActivation - The arctan function for type Float64","category":"page"},{"location":"train/general/#General-Training","page":"General Training","title":"General Training","text":"","category":"section"},{"location":"train/general/#arffsolver","page":"General Training","title":"Data Structure","text":"","category":"section"},{"location":"train/general/","page":"General Training","title":"General Training","text":"In the most general case, when a user calls train_arff! or train_arff, they must provide an ARFFSolver data structure:","category":"page"},{"location":"train/general/","page":"General Training","title":"General Training","text":"ARFF.ARFFSolver","category":"page"},{"location":"train/general/#ARFF.ARFFSolver","page":"General Training","title":"ARFF.ARFFSolver","text":"ARFFSolver{TS, TM, TR, TL}\n\nData structure containing key parameters for ARFF training\n\nFields\n\nlinear_solve! - User specified solver for the normal equations\nmutate! - In place transformation for the mutation/exploration step\nresample! - In place transformation for the resampling step\nloss - Loss function\n\n\n\n\n\n","category":"type"},{"location":"train/general/","page":"General Training","title":"General Training","text":"This includes stores the following functions:","category":"page"},{"location":"train/general/","page":"General Training","title":"General Training","text":"linear_solve!(β, ω, x, y, S, epoch): β and ω are the coefficients and wave numbers;  (x,y) are arrays of training pairs; S is the design matrix; and epoch is the current epoch.  Not all arguments are neccessarily used, but this is how the solver will be called.\nmutate!(F, x, y, S, epoch):  F is the Fourier model; (x,y) are arrays of training pairs; S is the design matrix; and epoch is the current epoch.\nresample!(F, x, y, S, epoch): This follows the convetion of mutate!\nloss(F, x, y): F is the Fourier model; and (x,y) are arrays of training pairs.","category":"page"},{"location":"train/general/#Training","page":"General Training","title":"Training","text":"","category":"section"},{"location":"train/general/","page":"General Training","title":"General Training","text":"These are underlying, general, training functions.  The user may wish to access these if they have custom mutation and resampling steps.  Otherwise, see, for instance Random Walk Metropolis Training which performs the algorithm described in [1]","category":"page"},{"location":"train/general/","page":"General Training","title":"General Training","text":"ARFF.train_arff!(F::ARFF.AbstractFourierModel, data_sets::TD, batch_size::TI, solver::ARFF.ARFFSolver, n_epochs::TI; show_progress=true, record_loss=true) where {TD,TI<:Integer}\nARFF.train_arff(F₀::ARFF.AbstractFourierModel, data_sets::TD, batch_size::TI, solver::ARFF.ARFFSolver, n_epochs::TI; show_progress=true, record_loss=true) where {TD,TI<:Integer}","category":"page"},{"location":"train/general/#ARFF.train_arff!-Union{Tuple{TI}, Tuple{TD}, Tuple{ARFF.AbstractFourierModel, TD, TI, ARFFSolver, TI}} where {TD, TI<:Integer}","page":"General Training","title":"ARFF.train_arff!","text":"train_arff!(F, data_sets, batch_size, solver, n_epochs; show_progress=true, record_loss=true)\n\nPerform RWM training in place on an ARFF model\n\nFields\n\nF - Fourier feature model to be trained, in place\ndata_sets - Training data sets \nbatch_size - Minibatch size\nsolver - An ARFFSolver data structure\nn_epochs - Number of training epochs\nshow_progress=true - Display training progress using ProgressMeter\nrecord_loss=true - Evaluate the specified loss function at each epoch and record\n\n\n\n\n\n","category":"method"},{"location":"train/general/#ARFF.train_arff-Union{Tuple{TI}, Tuple{TD}, Tuple{ARFF.AbstractFourierModel, TD, TI, ARFFSolver, TI}} where {TD, TI<:Integer}","page":"General Training","title":"ARFF.train_arff","text":"train_arff(F₀, data_sets, batch_size, solver, n_epochs; show_progress=true, record_loss=true)\n\nPerform RWM training an ARFF model, recording the result at all epochs.\n\nFields\n\nF₀ - Initial fourier feature model to be trained\ndata_sets - Training data sets \nbatch_size - Minibatch size\nsolver - An ARFFSolver data structure\nn_epochs - Number of training epochs\nshow_progress=true - Display training progress using ProgressMeter\nrecord_loss=true - Evaluate the specified loss function at each epoch and record\n\n\n\n\n\n","category":"method"},{"location":"train/general/","page":"General Training","title":"General Training","text":"For convenience, we have also included the following versions of these functions:","category":"page"},{"location":"train/general/","page":"General Training","title":"General Training","text":"ARFF.train_arff!(F::ARFF.ScalarFourierModel{TR,TY,TI,TA}, data::ARFF.ScalarDataSet{TR,TY,TI}, solver::ARFFSolver, n_epochs::TI; show_progress=true, record_loss=true) where {TY<:Number,TR<:AbstractFloat,TI<:Integer,TA<:ActivationFunction{TY}}\nARFF.train_arff!(F::ARFF.ScalarFourierModel{TR,TY,TI,TA}, data::ARFF.ScalarDataSet{TR,TY,TI}, batch_size::TI, solver::ARFFSolver, n_epochs::TI; show_progress=true, record_loss=true) where {TY<:Number,TR<:AbstractFloat,TI<:Integer,TA<:ActivationFunction{TY}}\nARFF.train_arff!(F::ARFF.ScalarFourierModel{TR,TY,TI,TA}, data_sets::Vector{ARFF.ScalarDataSet{TR,TY,TI}}, solver::ARFFSolver, n_epochs::TI; show_progress=true, record_loss=true) where {TY<:Number,TR<:AbstractFloat,TI<:Integer,TA<:ActivationFunction{TY}}\nARFF.train_arff(F₀::ARFF.ScalarFourierModel{TR,TY,TI,TA}, data::ARFF.ScalarDataSet{TR,TY,TI}, solver::ARFFSolver, n_epochs::TI; show_progress=true, record_loss=true) where {TY<:Number,TR<:AbstractFloat,TI<:Integer,TA<:ActivationFunction{TY}}\nARFF.train_arff(F₀::ARFF.ScalarFourierModel{TR,TY,TI,TA}, data::ARFF.ScalarDataSet{TR,TY,TI}, batch_size::TI, solver::ARFFSolver, n_epochs::TI; show_progress=true, record_loss=true) where {TY<:Number,TR<:AbstractFloat,TI<:Integer,TA<:ActivationFunction{TY}}\nARFF.train_arff(F₀::ARFF.ScalarFourierModel{TR,TY,TI,TA}, data_sets::Vector{ARFF.ScalarDataSet{TR,TY,TI}}, solver::ARFFSolver, n_epochs::TI; show_progress=true, record_loss=true) where {TY<:Number,TR<:AbstractFloat,TI<:Integer,TA<:ActivationFunction{TY}}","category":"page"},{"location":"train/general/#ARFF.train_arff!-Union{Tuple{TA}, Tuple{TI}, Tuple{TR}, Tuple{TY}, Tuple{ARFF.ScalarFourierModel{TR, TY, TI, TA}, ARFF.ScalarDataSet{TR, TY, TI}, ARFFSolver, TI}} where {TY<:Number, TR<:AbstractFloat, TI<:Integer, TA<:(ActivationFunction{TY})}","page":"General Training","title":"ARFF.train_arff!","text":"train_arff!(F, data, solver, n_epochs; show_progress=true, record_loss=true)\n\nPerform ARFF training on a given data set\n\nFields\n\nF - Fourier feature model to be trained, in place\ndata - Training data set\nsolver - An ARFFSolver data structure\nn_epochs - Number of training epochs\nshow_progress=true - Display training progress using ProgressMeter\nrecord_loss=true - Evaluate the specified loss function at each epoch and record\n\n\n\n\n\n","category":"method"},{"location":"train/general/#ARFF.train_arff!-Union{Tuple{TA}, Tuple{TI}, Tuple{TR}, Tuple{TY}, Tuple{ARFF.ScalarFourierModel{TR, TY, TI, TA}, ARFF.ScalarDataSet{TR, TY, TI}, TI, ARFFSolver, TI}} where {TY<:Number, TR<:AbstractFloat, TI<:Integer, TA<:(ActivationFunction{TY})}","page":"General Training","title":"ARFF.train_arff!","text":"train_arff!(F, data, solver, n_epochs; show_progress=true, record_loss=true)\n\nPerform ARFF training on a given data set with minibatching\n\nFields\n\nF - Fourier feature model to be trained, in place\ndata - Training data set\nbatch_size - Minibatch size\nsolver - An ARFFSolver data structure\nn_epochs - Number of training epochs\nshow_progress=true - Display training progress using ProgressMeter\nrecord_loss=true - Evaluate the specified loss function at each epoch and record\n\n\n\n\n\n","category":"method"},{"location":"train/general/#ARFF.train_arff!-Union{Tuple{TA}, Tuple{TI}, Tuple{TR}, Tuple{TY}, Tuple{ARFF.ScalarFourierModel{TR, TY, TI, TA}, Array{ARFF.ScalarDataSet{TR, TY, TI}, 1}, ARFFSolver, TI}} where {TY<:Number, TR<:AbstractFloat, TI<:Integer, TA<:(ActivationFunction{TY})}","page":"General Training","title":"ARFF.train_arff!","text":"train_arff!(F, data, solver, n_epochs; show_progress=true, record_loss=true)\n\nPerform ARFF training on a given data set with minibatching\n\nFields\n\nF - Fourier feature model to be trained, in place\ndata_sets - Array of training data sets\nsolver - An ARFFSolver data structure\nn_epochs - Number of training epochs\nshow_progress=true - Display training progress using ProgressMeter\nrecord_loss=true - Evaluate the specified loss function at each epoch and record\n\n\n\n\n\n","category":"method"},{"location":"train/general/#ARFF.train_arff-Union{Tuple{TA}, Tuple{TI}, Tuple{TR}, Tuple{TY}, Tuple{ARFF.ScalarFourierModel{TR, TY, TI, TA}, ARFF.ScalarDataSet{TR, TY, TI}, ARFFSolver, TI}} where {TY<:Number, TR<:AbstractFloat, TI<:Integer, TA<:(ActivationFunction{TY})}","page":"General Training","title":"ARFF.train_arff","text":"train_arff(F₀, data, solver, n_epochs; show_progress=true, record_loss=true)\n\nPerform RWM training an ARFF model, recording the result at all epochs.\n\nFields\n\nF₀ - Initial fourier feature model to be trained\ndata - Training data set\nsolver - An ARFFSolver data structure\nn_epochs - Number of training epochs\nshow_progress=true - Display training progress using ProgressMeter\nrecord_loss=true - Evaluate the specified loss function at each epoch and record\n\n\n\n\n\n","category":"method"},{"location":"train/general/#ARFF.train_arff-Union{Tuple{TA}, Tuple{TI}, Tuple{TR}, Tuple{TY}, Tuple{ARFF.ScalarFourierModel{TR, TY, TI, TA}, ARFF.ScalarDataSet{TR, TY, TI}, TI, ARFFSolver, TI}} where {TY<:Number, TR<:AbstractFloat, TI<:Integer, TA<:(ActivationFunction{TY})}","page":"General Training","title":"ARFF.train_arff","text":"train_arff(F₀, data, batch_size, solver, n_epochs; show_progress=true, record_loss=true)\n\nPerform RWM training an ARFF model, recording the result at all epochs.\n\nFields\n\nF₀ - Initial fourier feature model to be trained\ndata - Training data set\nbatch_size - Minibatch size\nsolver - An ARFFSolver data structure\nn_epochs - Number of training epochs\nshow_progress=true - Display training progress using ProgressMeter\nrecord_loss=true - Evaluate the specified loss function at each epoch and record\n\n\n\n\n\n","category":"method"},{"location":"train/general/#ARFF.train_arff-Union{Tuple{TA}, Tuple{TI}, Tuple{TR}, Tuple{TY}, Tuple{ARFF.ScalarFourierModel{TR, TY, TI, TA}, Array{ARFF.ScalarDataSet{TR, TY, TI}, 1}, ARFFSolver, TI}} where {TY<:Number, TR<:AbstractFloat, TI<:Integer, TA<:(ActivationFunction{TY})}","page":"General Training","title":"ARFF.train_arff","text":"train_arff(F₀, data_sets, solver, n_epochs; show_progress=true, record_loss=true)\n\nPerform RWM training an ARFF model, recording the result at all epochs.\n\nFields\n\nF₀ - Initial fourier feature model to be trained\ndata_sets - Array of training data sets\nsolver - An ARFFSolver data structure\nn_epochs - Number of training epochs\nshow_progress=true - Display training progress using ProgressMeter\nrecord_loss=true - Evaluate the specified loss function at each epoch and record\n\n\n\n\n\n","category":"method"},{"location":"train/rwm/#Random-Walk-Metropolis-Training","page":"Random Walk Metropolis Training","title":"Random Walk Metropolis Training","text":"","category":"section"},{"location":"train/rwm/","page":"Random Walk Metropolis Training","title":"Random Walk Metropolis Training","text":"For convenience, we have implemented the RWM strategy from [1].  ","category":"page"},{"location":"train/rwm/#RWM-Data-Structures","page":"Random Walk Metropolis Training","title":"RWM Data Structures","text":"","category":"section"},{"location":"train/rwm/#Classical-RWM","page":"Random Walk Metropolis Training","title":"Classical RWM","text":"","category":"section"},{"location":"train/rwm/","page":"Random Walk Metropolis Training","title":"Random Walk Metropolis Training","text":"For a classical RWM, we can employ a RWMSampler data structure, which can be constructed with one of the following commands:","category":"page"},{"location":"train/rwm/","page":"Random Walk Metropolis Training","title":"Random Walk Metropolis Training","text":"ARFF.RWMSampler(F::TF, linear_solve!::TS, n_rwm_steps::TI, δ::TR) where {TF<:ARFF.AbstractFourierModel,TS,TI<:Integer,TR<:AbstractFloat}\nARFF.RWMSampler(F::TF, linear_solve!::TS, n_rwm_steps::TI, Σ::TM, γ::TI, δ::TR, ω_max::TR) where {TF<:ARFF.AbstractFourierModel,TS,TI<:Integer,TM<:AbstractMatrix,TR<:AbstractFloat}","category":"page"},{"location":"train/rwm/#ARFF.RWMSampler-Union{Tuple{TR}, Tuple{TI}, Tuple{TS}, Tuple{TF}, Tuple{TF, TS, TI, TR}} where {TF<:ARFF.AbstractFourierModel, TS, TI<:Integer, TR<:AbstractFloat}","page":"Random Walk Metropolis Training","title":"ARFF.RWMSampler","text":"RWMSampler(F, linear_solve!, n_rwm_steps, δ)\n\nConstructor for the RWMSampler data structure.  Defaults to ω_max = Inf, γ = optimal_γ(dx), and Σ= I\n\nFields\n\nF - Fourier feature model; used for setting types and dimensions\nlinear_solve! - User specified solver for the normal equations\nn_rwm_steps - Number of internal RWM steps\nδ - RWM proposal step size\n\n\n\n\n\n","category":"method"},{"location":"train/rwm/#ARFF.RWMSampler-Union{Tuple{TR}, Tuple{TM}, Tuple{TI}, Tuple{TS}, Tuple{TF}, Tuple{TF, TS, TI, TM, TI, TR, TR}} where {TF<:ARFF.AbstractFourierModel, TS, TI<:Integer, TM<:(AbstractMatrix), TR<:AbstractFloat}","page":"Random Walk Metropolis Training","title":"ARFF.RWMSampler","text":"RWMSampler(F, linear_solve!, n_rwm_steps, Σ, γ, δ, ω_max)\n\nConstructor for the RWMSampler data structure\n\nFields\n\nF - Fourier feature model; used for setting types and dimensions\nlinear_solve! - User specified solver for the normal equations\nn_rwm_steps - Number of internal RWM steps\nΣ - Covariance matrix\nγ - Metropolis-Hastings exponent\nδ - RWM proposal step size\nω_max - Maximum wave number norm cutoff\n\n\n\n\n\n","category":"method"},{"location":"train/rwm/","page":"Random Walk Metropolis Training","title":"Random Walk Metropolis Training","text":"The linear_solve! argument should be of the same type as in ARFFSolver, `linear_solve!(β, ω, x, y, S, epoch); see Linear Algebra for additional details.  It is not neccessary to separately construct an ARFFSolver structure if using these.  These default to the mean squared error loss function, mse_loss.  As an exmaple, the following code is sufficient to construct such a structure, assuming one has already constructed an initial FourierModel, F0:","category":"page"},{"location":"train/rwm/","page":"Random Walk Metropolis Training","title":"Random Walk Metropolis Training","text":"using ARFF\nK = 64;\nF0 = FourierModel([1.0 * randn() for _ in 1:K], [[randn()] for _ in 1:K])","category":"page"},{"location":"train/rwm/","page":"Random Walk Metropolis Training","title":"Random Walk Metropolis Training","text":"δ = 1.0 # rwm step size\nn_rwm_steps = 10 # number of RWM steps within the mutation step\n\n# format the linear solver function to be compatible with ARFF.jl\nlinear_solver! = (β, ω, x, y, S, epoch) -> solve_normal!(β, S, y)\n\nrwm_sampler = RWMSampler(F0, linear_solver!, n_rwm_steps, δ);","category":"page"},{"location":"train/rwm/#Adaptive-RWM","page":"Random Walk Metropolis Training","title":"Adaptive RWM","text":"","category":"section"},{"location":"train/rwm/","page":"Random Walk Metropolis Training","title":"Random Walk Metropolis Training","text":"It may be more useful to use an AdaptiveRWMSampler data structure; this will update proposal covariance matrix after n_burn epochs.  This is the version used in our examples.","category":"page"},{"location":"train/rwm/","page":"Random Walk Metropolis Training","title":"Random Walk Metropolis Training","text":"The AdaptiveRWMSampler structure constructed with:","category":"page"},{"location":"train/rwm/","page":"Random Walk Metropolis Training","title":"Random Walk Metropolis Training","text":"ARFF.AdaptiveRWMSampler(F::TF, linear_solve!::TS, n_rwm_steps::TI, n_burn::TI, δ::TR) where {TF<:ARFF.AbstractFourierModel,TS,TI<:Integer,TR<:AbstractFloat}\nARFF.AdaptiveRWMSampler(F::TF, linear_solve!::TS, n_rwm_steps::TI, n_burn::TI, Σ0::TM, γ::TI, δ::TR, ω_max::TR) where {TF<:ARFF.AbstractFourierModel,TS,TI<:Integer,TM<:AbstractMatrix,TR<:AbstractFloat}","category":"page"},{"location":"train/rwm/#ARFF.AdaptiveRWMSampler-Union{Tuple{TR}, Tuple{TI}, Tuple{TS}, Tuple{TF}, Tuple{TF, TS, TI, TI, TR}} where {TF<:ARFF.AbstractFourierModel, TS, TI<:Integer, TR<:AbstractFloat}","page":"Random Walk Metropolis Training","title":"ARFF.AdaptiveRWMSampler","text":"AdaptiveRWMSampler(F, linear_solve!, n_rwm_steps, n_burn, δ)\n\nConstructor for the AdaptiveRWMSampler data structure. Defaults to ω_max = Inf, γ = optimal_γ(dx), and Σ0= I\n\nFields\n\nF - Fourier feature model; used for setting types and dimensions\nlinear_solve! - User specified solver for the normal equations\nn_rwm_steps - Number of internal RWM steps\nn_burn - Number of epochs before the covariance adaptation begins\nδ - RWM proposal step size\n\n\n\n\n\n","category":"method"},{"location":"train/rwm/#ARFF.AdaptiveRWMSampler-Union{Tuple{TR}, Tuple{TM}, Tuple{TI}, Tuple{TS}, Tuple{TF}, Tuple{TF, TS, TI, TI, TM, TI, TR, TR}} where {TF<:ARFF.AbstractFourierModel, TS, TI<:Integer, TM<:(AbstractMatrix), TR<:AbstractFloat}","page":"Random Walk Metropolis Training","title":"ARFF.AdaptiveRWMSampler","text":"AdaptiveRWMSampler(F, linear_solve!, n_rwm_steps, n_burn, Σ0, γ, δ, ω_max)\n\nConstructor for the AdaptiveRWMSampler data structure\n\nFields\n\nF - Fourier feature model; used for setting types and dimensions\nlinear_solve! - User specified solver for the normal equations\nn_rwm_steps - Number of internal RWM steps\nn_burn - Number of epochs before the covariance adaptation begins\nΣ0 - Initial covariance matrix\nγ - Metropolis-Hastings exponent\nδ - RWM proposal step size\nω_max - Maximum wave number norm cutoff\n\n\n\n\n\n","category":"method"},{"location":"train/rwm/","page":"Random Walk Metropolis Training","title":"Random Walk Metropolis Training","text":"As an example:","category":"page"},{"location":"train/rwm/","page":"Random Walk Metropolis Training","title":"Random Walk Metropolis Training","text":"using ARFF\nK = 64;\nF0 = FourierModel([1.0 * randn() for _ in 1:K], [[randn()] for _ in 1:K]);\nF = deepcopy(F0);\nn_epochs = 100;","category":"page"},{"location":"train/rwm/","page":"Random Walk Metropolis Training","title":"Random Walk Metropolis Training","text":"δ = 1.0 # rwm step size\nn_rwm_steps = 10 # number of RWM steps within the mutation step\nn_burn = 100 # number of training epochs to use for burn in.\n\n# format the linear solver function to be compatible with ARFF.jl\nlinear_solver! = (β, ω, x, y, S, epoch) -> solve_normal!(β, S, y)\n\nrwm_sampler = AdaptiveRWMSampler(F0, linear_solver!, n_rwm_steps, n_burn, δ); nothing","category":"page"},{"location":"train/rwm/#RWM-Mutation","page":"Random Walk Metropolis Training","title":"RWM Mutation","text":"","category":"section"},{"location":"train/rwm/","page":"Random Walk Metropolis Training","title":"Random Walk Metropolis Training","text":"As noted in General Training, the underlying training algorithm for ARFF requires the specification of a mutation step.  When we use RWM for that mutation step, as is automatically done for the user in RWM Training, and many of our examples, what we are actually doing is first constructing the mutation function, and then embedding it within an ARFFSolver data structure, as follows:","category":"page"},{"location":"train/rwm/","page":"Random Walk Metropolis Training","title":"Random Walk Metropolis Training","text":"mutate_rwm!(F, x, y, S, epoch) = ARFF.rwm!(F0, rwm_sampler, x, y, S, epoch);\nsolver = ARFFSolver(rwm_sampler.linear_solve!, mutate_rwm!, trivial_resample!, mse_loss); nothing","category":"page"},{"location":"train/rwm/","page":"Random Walk Metropolis Training","title":"Random Walk Metropolis Training","text":"This example assumes, of course, that you have already constructed the rwm_sample object.","category":"page"},{"location":"train/rwm/#RWM-Training","page":"Random Walk Metropolis Training","title":"RWM Training","text":"","category":"section"},{"location":"train/rwm/","page":"Random Walk Metropolis Training","title":"Random Walk Metropolis Training","text":"RWM training is implemented to handle the training data in several ways:","category":"page"},{"location":"train/rwm/","page":"Random Walk Metropolis Training","title":"Random Walk Metropolis Training","text":"A single DataSet can be provided and used in every training epoch.\nA single DataSet and a minibatch size can be provided, and minibatchs will be generated at each epoch.\nAn array of DataSet types can be provided, and they will be cycled through each epoch.","category":"page"},{"location":"train/rwm/","page":"Random Walk Metropolis Training","title":"Random Walk Metropolis Training","text":"Additionally, we have both an in place training commands, along with commands which will record the training trajectory.  These can be used for both scalar and vector valued problems.","category":"page"},{"location":"train/rwm/","page":"Random Walk Metropolis Training","title":"Random Walk Metropolis Training","text":"ARFF.train_rwm!(F::ARFF.ScalarFourierModel{TR,TY,TI,TA}, data::ARFF.ScalarDataSet{TR,TY,TI}, rwm_sampler::TS, n_epochs::TI; show_progress=true, record_loss=true) where {TY<:Number,TR<:AbstractFloat,TI<:Integer,TA<:ARFF.ActivationFunction{TY},TS<:ARFF.AdaptiveRWMSampler}\nARFF.train_rwm!(F::ARFF.ScalarFourierModel{TR,TY,TI,TA}, data::ARFF.ScalarDataSet{TR,TY,TI}, batch_size::TI, rwm_sampler::TS, n_epochs::TI; show_progress=true, record_loss=true) where {TY<:Number,TR<:AbstractFloat,TI<:Integer,TA<:ARFF.ActivationFunction{TY},TS<:ARFF.AdaptiveRWMSampler}\nARFF.train_rwm!(F::ARFF.ScalarFourierModel{TR,TY,TI,TA}, data_sets::Vector{ARFF.ScalarDataSet{TR,TY,TI}}, rwm_sampler::TS, n_epochs::TI; show_progress=true, record_loss=true) where {TY<:Number,TR<:AbstractFloat,TI<:Integer,TA<:ARFF.ActivationFunction{TY},TS<:ARFF.AdaptiveRWMSampler} ","category":"page"},{"location":"train/rwm/#ARFF.train_rwm!-Union{Tuple{TS}, Tuple{TA}, Tuple{TI}, Tuple{TR}, Tuple{TY}, Tuple{ARFF.ScalarFourierModel{TR, TY, TI, TA}, ARFF.ScalarDataSet{TR, TY, TI}, TS, TI}} where {TY<:Number, TR<:AbstractFloat, TI<:Integer, TA<:(ActivationFunction{TY}), TS<:AdaptiveRWMSampler}","page":"Random Walk Metropolis Training","title":"ARFF.train_rwm!","text":"train_rwm!(F, data, rwm_sampler; show_progress=true, record_loss=true)\n\nPerform RWM training of an ARFF model\n\nFields\n\nF - Fourier feature model to be trained, in place\ndata - Training data \nrwm_sampler - An RWM sampler data structure\nn_epochs - Number of training epochs\nshow_progress=true - Display training progress using ProgressMeter\nrecord_loss=true - Evaluate the specified loss function at each epoch and record\n\n\n\n\n\n","category":"method"},{"location":"train/rwm/#ARFF.train_rwm!-Union{Tuple{TS}, Tuple{TA}, Tuple{TI}, Tuple{TR}, Tuple{TY}, Tuple{ARFF.ScalarFourierModel{TR, TY, TI, TA}, ARFF.ScalarDataSet{TR, TY, TI}, TI, TS, TI}} where {TY<:Number, TR<:AbstractFloat, TI<:Integer, TA<:(ActivationFunction{TY}), TS<:AdaptiveRWMSampler}","page":"Random Walk Metropolis Training","title":"ARFF.train_rwm!","text":"train_rwm!(F, data, batch_size, rwm_sampler, n_epochs; show_progress=true, record_loss=true)\n\nPerform RWM training of an ARFF model with mini batching\n\nFields\n\nF - Fourier feature model to be trained, in place\ndata - Training data \nbatch_size - Minibatch size\nrwm_sampler - An RWM sampler data structure\nn_epochs - Number of training epochs\nshow_progress=true - Display training progress using ProgressMeter\nrecord_loss=true - Evaluate the specified loss function at each epoch and record\n\n\n\n\n\n","category":"method"},{"location":"train/rwm/#ARFF.train_rwm!-Union{Tuple{TS}, Tuple{TA}, Tuple{TI}, Tuple{TR}, Tuple{TY}, Tuple{ARFF.ScalarFourierModel{TR, TY, TI, TA}, Array{ARFF.ScalarDataSet{TR, TY, TI}, 1}, TS, TI}} where {TY<:Number, TR<:AbstractFloat, TI<:Integer, TA<:(ActivationFunction{TY}), TS<:AdaptiveRWMSampler}","page":"Random Walk Metropolis Training","title":"ARFF.train_rwm!","text":"train_rwm!(F, data, batch_size, rwm_sampler, n_epochs; show_progress=true, record_loss=true)\n\nPerform RWM training of an ARFF model with an array of different training data sets\n\nFields\n\nF - Fourier feature model to be trained, in place\ndata_sets - Training data sets \nrwm_sampler - An RWM sampler data structure\nn_epochs - Number of training epochs\nshow_progress=true - Display training progress using ProgressMeter\nrecord_loss=true - Evaluate the specified loss function at each epoch and record\n\n\n\n\n\n","category":"method"},{"location":"train/rwm/","page":"Random Walk Metropolis Training","title":"Random Walk Metropolis Training","text":"Having created an initial F we can then call","category":"page"},{"location":"train/rwm/","page":"Random Walk Metropolis Training","title":"Random Walk Metropolis Training","text":"acceptance_rate, loss = train_rwm!(F, data, rwm_sampler, n_epochs);","category":"page"},{"location":"train/rwm/","page":"Random Walk Metropolis Training","title":"Random Walk Metropolis Training","text":"The acceptance_rate is the mean acceptance rate at each epoch, averaged over the internal steps,  K * n_rwm_steps.  The loss is the recorded training loss at each epoch.  If using AdaptiveRWMSampler, the tuned covariance can be obtained by examining","category":"page"},{"location":"train/rwm/","page":"Random Walk Metropolis Training","title":"Random Walk Metropolis Training","text":"rwm_sampler.Σ_mean","category":"page"},{"location":"train/rwm/","page":"Random Walk Metropolis Training","title":"Random Walk Metropolis Training","text":"ARFF.train_rwm(F₀::ARFF.ScalarFourierModel{TR,TY,TI,TA}, data::ARFF.ScalarDataSet{TR,TY,TI}, rwm_sampler::TS, n_epochs::TI; show_progress=true, record_loss=true) where {TY<:Number,TR<:AbstractFloat,TI<:Integer,TA<:ARFF.ActivationFunction{TY},TS<:ARFF.AdaptiveRWMSampler}\nARFF.train_rwm(F₀::ARFF.ScalarFourierModel{TR,TY,TI,TA}, data::ARFF.ScalarDataSet{TR,TY,TI}, batch_size::TI, rwm_sampler::TS, n_epochs::TI; show_progress=true, record_loss=true) where {TY<:Number,TR<:AbstractFloat,TI<:Integer,TA<:ARFF.ActivationFunction{TY},TS<:ARFF.AdaptiveRWMSampler}\nARFF.train_rwm(F₀::ARFF.ScalarFourierModel{TR,TY,TI,TA}, data_sets::Vector{ARFF.ScalarDataSet{TR,TY,TI}}, rwm_sampler::TS, n_epochs::TI; show_progress=true, record_loss=true) where {TY<:Number,TR<:AbstractFloat,TI<:Integer,TA<:ARFF.ActivationFunction{TY},TS<:ARFF.AdaptiveRWMSampler} ","category":"page"},{"location":"train/rwm/#ARFF.train_rwm-Union{Tuple{TS}, Tuple{TA}, Tuple{TI}, Tuple{TR}, Tuple{TY}, Tuple{ARFF.ScalarFourierModel{TR, TY, TI, TA}, ARFF.ScalarDataSet{TR, TY, TI}, TS, TI}} where {TY<:Number, TR<:AbstractFloat, TI<:Integer, TA<:(ActivationFunction{TY}), TS<:AdaptiveRWMSampler}","page":"Random Walk Metropolis Training","title":"ARFF.train_rwm","text":"train_rwm(F₀, data, rwm_sampler; show_progress=true, record_loss=true)\n\nPerform RWM training of an ARFF model\n\nFields\n\nF₀ - Initial Fourier feature model to be trained\ndata - Training data \nrwm_sampler - An RWM sampler data structure\nn_epochs - Number of training epochs\nshow_progress=true - Display training progress using ProgressMeter\nrecord_loss=true - Evaluate the specified loss function at each epoch and record\n\n\n\n\n\n","category":"method"},{"location":"train/rwm/#ARFF.train_rwm-Union{Tuple{TS}, Tuple{TA}, Tuple{TI}, Tuple{TR}, Tuple{TY}, Tuple{ARFF.ScalarFourierModel{TR, TY, TI, TA}, ARFF.ScalarDataSet{TR, TY, TI}, TI, TS, TI}} where {TY<:Number, TR<:AbstractFloat, TI<:Integer, TA<:(ActivationFunction{TY}), TS<:AdaptiveRWMSampler}","page":"Random Walk Metropolis Training","title":"ARFF.train_rwm","text":"train_rwm(F₀, data, batch_size, rwm_sampler, n_epochs; show_progress=true, record_loss=true)\n\nPerform RWM training of an ARFF model with mini batching\n\nFields\n\nF₀ - Initial Fourier feature model to be trained\ndata - Training data \nbatch_size - Minibatch size\nrwm_sampler - An RWM sampler data structure\nn_epochs - Number of training epochs\nshow_progress=true - Display training progress using ProgressMeter\nrecord_loss=true - Evaluate the specified loss function at each epoch and record\n\n\n\n\n\n","category":"method"},{"location":"train/rwm/#ARFF.train_rwm-Union{Tuple{TS}, Tuple{TA}, Tuple{TI}, Tuple{TR}, Tuple{TY}, Tuple{ARFF.ScalarFourierModel{TR, TY, TI, TA}, Array{ARFF.ScalarDataSet{TR, TY, TI}, 1}, TS, TI}} where {TY<:Number, TR<:AbstractFloat, TI<:Integer, TA<:(ActivationFunction{TY}), TS<:AdaptiveRWMSampler}","page":"Random Walk Metropolis Training","title":"ARFF.train_rwm","text":"train_rwm(F₀, data, batch_size, rwm_sampler, n_epochs; show_progress=true, record_loss=true)\n\nPerform RWM training of an ARFF model with an array of different training data sets\n\nFields\n\nF₀ - Initial Fourier feature model to be trained\ndata_sets - Training data sets \nrwm_sampler - An RWM sampler data structure\nn_epochs - Number of training epochs\nshow_progress=true - Display training progress using ProgressMeter\nrecord_loss=true - Evaluate the specified loss function at each epoch and record\n\n\n\n\n\n","category":"method"},{"location":"train/rwm/","page":"Random Walk Metropolis Training","title":"Random Walk Metropolis Training","text":"Having created an initial F₀ we can then call","category":"page"},{"location":"train/rwm/","page":"Random Walk Metropolis Training","title":"Random Walk Metropolis Training","text":"F_trajecotry, acceptance_rate, loss = train_rwm(FF₀, data, rwm_sampler, n_epochs);","category":"page"},{"location":"train/rwm/","page":"Random Walk Metropolis Training","title":"Random Walk Metropolis Training","text":"One of the returned quantities is the algorithmic trajectory, F_trajecotry. The acceptance_rate is the mean acceptance rate at each epoch, averaged overa the internal steps,  K * n_rwm_steps.  The loss is the recorded training loss at each epoch.","category":"page"},{"location":"examples/example4/#Training-with-Resampling","page":"Training with Resampling","title":"Training with Resampling","text":"","category":"section"},{"location":"examples/example4/","page":"Training with Resampling","title":"Training with Resampling","text":"To illustrate the use of resampling, we follow an example from Section 3 of  [2].  We will fit data generated by the following function f mathbbR^d to mathbbR:","category":"page"},{"location":"examples/example4/","page":"Training with Resampling","title":"Training with Resampling","text":"beginalign*\n    boldsymbolx_m sim mathcalN(boldsymbol0 Id) \n    y_m = f(boldsymbolx_m) \n    f(boldsymbolx) = mathrmSileft(fracboldsymbolxB_1aright)expleft(-fracboldsymbolxB_2^22right)\nendalign*","category":"page"},{"location":"examples/example4/","page":"Training with Resampling","title":"Training with Resampling","text":"for row vector boldsymbolx_m in mathbbR^1 times d and where boldsymbolxB_1 denotes the first component of the vector boldsymbolxB with B being the rotation matrix defined as follows:","category":"page"},{"location":"examples/example4/","page":"Training with Resampling","title":"Training with Resampling","text":"B = beginbmatrix\n08617  04975  -00998  -00000 \n03028  -05246  -00000  07957 \n00865  00499  09950  00000 \n03978  -06891  -00000  -06057 \nendbmatrix","category":"page"},{"location":"examples/example4/","page":"Training with Resampling","title":"Training with Resampling","text":"In our example, we will use a = 10^-2.","category":"page"},{"location":"examples/example4/","page":"Training with Resampling","title":"Training with Resampling","text":"using ARFF\nusing Random\nusing Statistics\nusing LinearAlgebra\nusing SpecialFunctions\nusing Plots\nusing Distributions","category":"page"},{"location":"examples/example4/#Define-Function","page":"Training with Resampling","title":"Define Function","text":"","category":"section"},{"location":"examples/example4/","page":"Training with Resampling","title":"Training with Resampling","text":"\nBt = [  0.8617 0.4975 -0.0998 -0.0;\n        0.3028 -0.5246 -0.0000 0.7957;\n        0.0865 0.0499 0.9950 0.0000;\n        0.3978 -0.6891 -0.0000 -0.6057];\n\nB = (Bt')|>collect;\nb1 = B[1,:];\n\na = 1e-2;\nf(x) = sinint(b1⋅x/ a) * exp(-0.5 * (norm(B*x,2)^2)); nothing;","category":"page"},{"location":"examples/example4/#Generate-data","page":"Training with Resampling","title":"Generate data","text":"","category":"section"},{"location":"examples/example4/","page":"Training with Resampling","title":"Training with Resampling","text":"Next, we generate our training data.","category":"page"},{"location":"examples/example4/","page":"Training with Resampling","title":"Training with Resampling","text":"dx = 4;\nnx = 10^4;\nRandom.seed!(100) # for reproducibility\nx = [randn(dx) for _ in 1:nx];\ny = f.(x);\ndata = DataSet(x, y); nothing;","category":"page"},{"location":"examples/example4/#Define-Training-Parameters","page":"Training with Resampling","title":"Define Training Parameters","text":"","category":"section"},{"location":"examples/example4/","page":"Training with Resampling","title":"Training with Resampling","text":"We now define our model and prepare for training.  Note that this example uses sigmoid type activation functions.","category":"page"},{"location":"examples/example4/","page":"Training with Resampling","title":"Training with Resampling","text":"@show K = 2^8;\nRandom.seed!(200) # for reproducibility\n\nF0 = FourierModel([1.0 for _ in 1:K], [10 * rand(dx) for _ in 1:K], SigmoidActivation)\nδ = 0.1 # rwm step size\nλ = 1e-1 # regularization\nn_epochs = 10^3 # number of epochs\n# number of rwm steps between full updates and resampling\nn_rwm_steps = [5, 5, 1] \n# use 10% of the run for burn in\nn_burn = n_epochs ÷ 10 \n\nbatch_size = 10^3; nothing;","category":"page"},{"location":"examples/example4/#Define-solver-and-resampler-for-each-R-value","page":"Training with Resampling","title":"Define solver and resampler for each R value","text":"","category":"section"},{"location":"examples/example4/","page":"Training with Resampling","title":"Training with Resampling","text":"We now define the functions we intend to use, including a distinct resampler depending on the value of R:","category":"page"},{"location":"examples/example4/","page":"Training with Resampling","title":"Training with Resampling","text":"linear_solver! = (β, ω, x, y, S, epoch) -> solve_normal!(β, S, y, λ=λ) # define linear solver\n\nrwm_sampler = [AdaptiveRWMSampler(F0, linear_solver!, n_rwm_steps_, n_burn, δ) for n_rwm_steps_ in n_rwm_steps]; #define rwm sampler for each R.\n\nR_vals = [0.0, 0.75, 1.0] # effective sample size thresholds\n\nmutate_rwm1!(F, x, y, S, n) = ARFF.rwm!(F, rwm_sampler[1], x, y, S, n) # mutate rwm for R=0.0\nmutate_rwm2!(F, x, y, S, n) = ARFF.rwm!(F, rwm_sampler[2], x, y, S, n) # mutate rwm for R=0.75\nmutate_rwm3!(F, x, y, S, n) = ARFF.rwm!(F, rwm_sampler[3], x, y, S, n) # mutate rwm for R=1.0\n\nmutate_rwm_array = [mutate_rwm1!, mutate_rwm2!, mutate_rwm3!]\n\nresampler! = [(F, x, y, S, epoch) -> ARFF.resample!(F, x, y, S, epoch, rwm_sampler[i].linear_solve!, R=R_vals[i]) for i in 1:3] # resampler for every R value\n\nsolver = [ARFFSolver(rwm_sampler[i].linear_solve!, mutate_rwm_array[i], resampler![i], mse_loss) for i in 1:3]; # solver for every resampler \nnothing;","category":"page"},{"location":"examples/example4/#Train-and-Evaluate-Results","page":"Training with Resampling","title":"Train and Evaluate Results","text":"","category":"section"},{"location":"examples/example4/#Train-R0.0-Network","page":"Training with Resampling","title":"Train R=00 Network","text":"","category":"section"},{"location":"examples/example4/","page":"Training with Resampling","title":"Training with Resampling","text":"This example corresponds to pure RWM; no resampling takes place.","category":"page"},{"location":"examples/example4/","page":"Training with Resampling","title":"Training with Resampling","text":"Random.seed!(1000) # for reproducibility\nG1 = deepcopy(F0)\nloss1 = train_arff!(G1,data, batch_size, solver[1], n_epochs, show_progress=false);nothing;","category":"page"},{"location":"examples/example4/#Train-R0.8-Network","page":"Training with Resampling","title":"Train R=08 Network","text":"","category":"section"},{"location":"examples/example4/","page":"Training with Resampling","title":"Training with Resampling","text":"In this case, resampling takes place when the ESS falls beneath R cdot K.","category":"page"},{"location":"examples/example4/","page":"Training with Resampling","title":"Training with Resampling","text":"Random.seed!(1000) # for reproducibility\nG2 = deepcopy(F0)\nloss2 = train_arff!(G2, data, batch_size, solver[2], n_epochs, show_progress=false); nothing;","category":"page"},{"location":"examples/example4/#Train-R1.0-Network","page":"Training with Resampling","title":"Train R=10 Network","text":"","category":"section"},{"location":"examples/example4/","page":"Training with Resampling","title":"Training with Resampling","text":"In this case, resampling takes place at every step.","category":"page"},{"location":"examples/example4/","page":"Training with Resampling","title":"Training with Resampling","text":"Random.seed!(1000) # for reproducibility\nG3 = deepcopy(F0)\nloss3 = train_arff!(G3, data, batch_size, solver[3], n_epochs, show_progress=false); nothing;","category":"page"},{"location":"examples/example4/#Compare-Loss-Curves","page":"Training with Resampling","title":"Compare Loss Curves","text":"","category":"section"},{"location":"examples/example4/","page":"Training with Resampling","title":"Training with Resampling","text":"plot(1:n_epochs, loss1, xscale=:log10, yscale = :log10, label = \"R=0.0\", legend = :bottomleft)\nplot!(1:n_epochs, loss2, label = \"R=0.75\")\nplot!(1:n_epochs, loss3, label = \"R=1.0\")\nxlabel!(\"Epoch\")\nylabel!(\"Loss\")","category":"page"},{"location":"examples/example4/","page":"Training with Resampling","title":"Training with Resampling","text":"The parameters can be tuned in the above example to get more accurate results. We have found resampling method improves some problems, but performs similarily to the R=00 case in others. The performance appears to improve the most when using large data sets with large K values.  See [2] for more examples.","category":"page"},{"location":"#ARFF.jl-Documentation","page":"Home","title":"ARFF.jl Documentation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for the adaptive random fourier features (ARFF) package.  This package is built around the methodology presented in [1].","category":"page"},{"location":"","page":"Home","title":"Home","text":"Using the package involves three steps:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Formatting your training data into a DataSet structure\nInitializing a FourierModel  structure\nTraining","category":"page"},{"location":"#Overview","page":"Home","title":"Overview","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The essential idea of ARFF is to make an approximation of a true function, f^daggermathbbR^dto mathbbC, as","category":"page"},{"location":"","page":"Home","title":"Home","text":"f^dagger(x) approx f(x) = sum_k=1^K beta_k e^i omega_k cdot x","category":"page"},{"location":"","page":"Home","title":"Home","text":"where xomega_kin mathbbR^d, while beta_kinmathbbC.  In the naive random Fourier featuer setting, the omega_k are sampled from some known distribution mu, and the beta_k are obtained by classical least squares regression or ridge regression,","category":"page"},{"location":"","page":"Home","title":"Home","text":"(S^astS + N lambda I)boldsymbolbeta = S^ast boldsymboly","category":"page"},{"location":"","page":"Home","title":"Home","text":"where the design matrix, S, is Ntimes K, with entries","category":"page"},{"location":"","page":"Home","title":"Home","text":"S_jk = e^ i omega_k cdot x_j","category":"page"},{"location":"","page":"Home","title":"Home","text":"We presume that we have training data of size N, (x_jy_j)_j=1^N. Other solutions are possible.","category":"page"},{"location":"","page":"Home","title":"Home","text":"This package generalizes the method to allow for both vector valued functions and permit activation functions other than the complex exponential:","category":"page"},{"location":"","page":"Home","title":"Home","text":"f(x) = sum_k=1^K beta_k varphi(xomega_k)","category":"page"},{"location":"","page":"Home","title":"Home","text":"where now beta_k in mathbbR^d or beta_k in mathbbC^d, where d need not be the same as d.","category":"page"},{"location":"#Adaptivity","page":"Home","title":"Adaptivity","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To make the algorithm adaptive, that is to say, to sample the frequencies from an optimal distribution, we use a Random Walk Metropolis scheme described in [1].  The goal is to sample from the variance minimizing distribution, known to be propto hatf(omega).","category":"page"},{"location":"","page":"Home","title":"Home","text":"The strategy is as follows:","category":"page"},{"location":"#Generate-Proposal","page":"Home","title":"Generate Proposal","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Perturb the vector boldsymbolomega of wave numbers with a Gaussian,","category":"page"},{"location":"","page":"Home","title":"Home","text":"boldsymbolomega = boldsymbolomega + delta boldsymbolxi quad boldsymbolxisim N(0 Sigma)","category":"page"},{"location":"","page":"Home","title":"Home","text":"where delta0 is a proposal step size and  Sigma is a covariance matrix. ","category":"page"},{"location":"#Update-Amplitudes","page":"Home","title":"Update Amplitudes","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Compute the proposed amplitudes, boldsymbolbeta for the perturbed wave numbers, by building up the new design matrix and solving the linear system. ","category":"page"},{"location":"#Accept/Reject","page":"Home","title":"Accept/Reject","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Accept/reject each wave vector omega_k with probability","category":"page"},{"location":"","page":"Home","title":"Home","text":"minleft1 fracbeta_k^gammabeta_k^gammaright","category":"page"},{"location":"","page":"Home","title":"Home","text":"where gamma0 is a tuning parameter that plays a role anlogous to inverse temperature.","category":"page"},{"location":"#Training","page":"Home","title":"Training","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Having described a single the RWM step, the core of ARFF training requires  a total number of epochs (n_epochs) and number of RWM steps (n_rwm_steps).  The core of the training loop consists of:","category":"page"},{"location":"","page":"Home","title":"Home","text":"for i in 1:n_epochs\n    # solve for β with current ω\n    for j in 1:n_ω_steps\n        # generate an RWM proposal\n            for k in 1:K\n                # accept/reject each ω_k\n            end\n    end\nend","category":"page"},{"location":"","page":"Home","title":"Home","text":"In the current version of ARFF.jl, the RWM piece is encapsulated inside of a mutate! operation, which can be modified as the user desires.","category":"page"},{"location":"#Notes-on-Current-Training-Implementation","page":"Home","title":"Notes on Current Training Implementation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package has undergone several revisions.  In the current form, one can specify two actions:","category":"page"},{"location":"","page":"Home","title":"Home","text":"resample! - The resampling step resamples amongs the current set of wave numbers.  Resampling was introduced to the training method for ARFF in [2].\nmutate! - The mutation step is what discovers new weave numbers.  In the","category":"page"},{"location":"","page":"Home","title":"Home","text":"current implementation, this corresponds to the RWM step.  These two procedures are embedded within a ARFFSolver structure. Furthermore, it is assumed that both of these procedures are implemented so as to conclude by calling the linear solver to obtain the updated boldsymbolbeta. ","category":"page"},{"location":"#Acknowledgements","page":"Home","title":"Acknowledgements","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Contributors to this project include:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Gideon Simpson \nPetr Plechac\nJerome Troy\nLiam Doherty\nHunter Wages","category":"page"},{"location":"","page":"Home","title":"Home","text":"This work was supported in part by the ARO Cooperative Agreement W911NF2220234.","category":"page"},{"location":"#References","page":"Home","title":"References","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A. Kammonen, J. Kiessling, P. Plecháč, M. Sandberg and A. Szepessy. Adaptive random Fourier features with Metropolis sampling. Foundations of Data Science 2, 309–332 (2020). Accessed on Nov 18, 2023.\n\n\n\nA. Kammonen, A. Pandey, E. v. Schwerin and R. Tempone. Adaptive Random Fourier Features Training Stabilized By Resampling With Applications in Image Regression (Oct 2024). Accessed on Oct 11, 2024, arXiv:2410.06399.\n\n\n\nJ. Kiessling, E. Ström and R. Tempone. Wind field reconstruction with adaptive random Fourier features. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences 477, 20210236 (2021). Accessed on May 5, 2023.\n\n\n\nA. Kammonen, J. Kiessling, P. Plecháč, M. Sandberg, A. Szepessy and R. Tempone. Smaller generalization error derived for a deep residual neural network compared with shallow networks. IMA Journal of Numerical Analysis 43, 2585–2632 (2023). Accessed on Nov 12, 2023.\n\n\n\n","category":"page"}]
}
