var documenterSearchIndex = {"docs":
[{"location":"examples/example3/#Example-with-Generalized-Activation-Functions-and-Scalings","page":"Example with Generalized Activation Functions and Scalings","title":"Example with Generalized Activation Functions and Scalings","text":"","category":"section"},{"location":"examples/example3/","page":"Example with Generalized Activation Functions and Scalings","title":"Example with Generalized Activation Functions and Scalings","text":"This example demonstates learning with generalized activation functions, a bias term, and scalings.  For this problem, ","category":"page"},{"location":"examples/example3/","page":"Example with Generalized Activation Functions and Scalings","title":"Example with Generalized Activation Functions and Scalings","text":"f(x) = expleft(-frac12left(x_1^2 + tfrac1100x_2^2 right)right)","category":"page"},{"location":"examples/example3/","page":"Example with Generalized Activation Functions and Scalings","title":"Example with Generalized Activation Functions and Scalings","text":"As was the case in Scalar Example with Generalized Activation Functions, we will make use of sigmoid activations and augment the problem with a bias. What is a bit different here is that we will also scale the data.","category":"page"},{"location":"examples/example3/#Generate-Training-Data","page":"Example with Generalized Activation Functions and Scalings","title":"Generate Training Data","text":"","category":"section"},{"location":"examples/example3/","page":"Example with Generalized Activation Functions and Scalings","title":"Example with Generalized Activation Functions and Scalings","text":"using Random\nusing Distributions\nusing Plots\nusing LinearAlgebra\nusing ARFF","category":"page"},{"location":"examples/example3/","page":"Example with Generalized Activation Functions and Scalings","title":"Example with Generalized Activation Functions and Scalings","text":"First, we will generate and visualize the training data:","category":"page"},{"location":"examples/example3/","page":"Example with Generalized Activation Functions and Scalings","title":"Example with Generalized Activation Functions and Scalings","text":"f(x) = exp(-0.5 * (x[1]^2+ x[2]^2/100));\n\nn_x = 1000; # number of training points\n\nRandom.seed!(100); # for reproducibility\n# generate n_x sample points\nd_ = 2;\nx = [[rand(Uniform(-5.0, 5.0)), rand(Uniform(-50.0, 50.0))] for _ in 1:n_x];\ny = f.(x);\n\n# store data in DataSet structure\ndata_ = DataSet(x,y);\n# scale the data\nscalings_ = get_scalings(data_);\nscale_data!(data_, scalings_)\n# append the bias term to our data and scalings\ndata = append_bias(data_);\nscalings = append_bias(scalings_);\nd  = d_ + 1; nothing","category":"page"},{"location":"examples/example3/#Initialize-Fourier-Model-with-Generalized-Features","page":"Example with Generalized Activation Functions and Scalings","title":"Initialize Fourier Model with Generalized Features","text":"","category":"section"},{"location":"examples/example3/","page":"Example with Generalized Activation Functions and Scalings","title":"Example with Generalized Activation Functions and Scalings","text":"Next, we need to initialize our FourierModel","category":"page"},{"location":"examples/example3/","page":"Example with Generalized Activation Functions and Scalings","title":"Example with Generalized Activation Functions and Scalings","text":"K = 2^7;\nRandom.seed!(200); # for reproducibility\nF0 = FourierModel([1. *randn() for _ in 1:K],  \n    [randn(d) for _ in 1:K],SigmoidActivation); nothing","category":"page"},{"location":"examples/example3/#Set-Parameters-and-Train","page":"Example with Generalized Activation Functions and Scalings","title":"Set Parameters and Train","text":"","category":"section"},{"location":"examples/example3/","page":"Example with Generalized Activation Functions and Scalings","title":"Example with Generalized Activation Functions and Scalings","text":"δ = 0.1; # rwm step size\nλ = 1e-6; # regularization\nn_epochs = 10^3; # total number of iterations\nn_ω_steps = 10; # number of steps between full β updates\nn_burn = n_epochs ÷ 10;\nγ = optimal_γ(d);\nω_max =Inf;\nadapt_covariance = true;\n\nΣ0 = diagm(ones(d));\n\nβ_solver! = (β, S, y, ω)-> solve_normal!(β, S, y, λ=λ);\n\nopts = ARFFOptions(n_epochs, n_ω_steps, δ, n_burn, γ, ω_max,adapt_covariance, \n    β_solver!, ARFF.mse_loss);\n\nRandom.seed!(1000); # for reproducibility\nF = deepcopy(F0);\nΣ_mean, acceptance_rate, loss= train_rwm!(F, data, Σ0, opts, show_progress=false); nothing ","category":"page"},{"location":"examples/example3/#Evaluate-Results","page":"Example with Generalized Activation Functions and Scalings","title":"Evaluate Results","text":"","category":"section"},{"location":"examples/example3/","page":"Example with Generalized Activation Functions and Scalings","title":"Example with Generalized Activation Functions and Scalings","text":"Looking at the trianing loss, we see the model appears to be well trained for the selected width, K:","category":"page"},{"location":"examples/example3/","page":"Example with Generalized Activation Functions and Scalings","title":"Example with Generalized Activation Functions and Scalings","text":"plot(1:length(loss), loss, yscale=:log10, label=\"\")\nxlabel!(\"Epoch\")\nylabel!(\"Loss\")","category":"page"},{"location":"examples/example3/","page":"Example with Generalized Activation Functions and Scalings","title":"Example with Generalized Activation Functions and Scalings","text":"We verify that the training data is well fit:","category":"page"},{"location":"examples/example3/","page":"Example with Generalized Activation Functions and Scalings","title":"Example with Generalized Activation Functions and Scalings","text":"scatter(data.y,F.(data.x),label=\"Data\")\nxx = LinRange(minimum(data.y),maximum(data.y),100);\nplot!(xx, xx, ls=:dash, label=\"\")\nxlabel!(\"Truth\")\nylabel!(\"Prediction\")","category":"page"},{"location":"examples/example3/","page":"Example with Generalized Activation Functions and Scalings","title":"Example with Generalized Activation Functions and Scalings","text":"Lastly, a direct comparison of the learned approximation of f:","category":"page"},{"location":"examples/example3/","page":"Example with Generalized Activation Functions and Scalings","title":"Example with Generalized Activation Functions and Scalings","text":"xx = LinRange(-2, 2, 100)\nyy = LinRange(-20, 20, 100)\n\np1 = contourf(xx, yy, [f([x_, y_]) for y_ in yy, x_ in xx],levels=LinRange(-0.1,1.1,20), colorbar=:false)\nxlabel!(p1, \"x\")\nylabel!(p1, \"y\")\ntitle!(p1, \"Truth\")\np2 = contourf(xx, yy, [F([x_, y_, 1], scalings) for y_ in yy, x_ in xx], levels=LinRange(-0.1, 1.1, 20), colorbar=:false)\nxlabel!(p2, \"x\")\ntitle!(p2, \"Learned Model\")\nplot(p1, p2, layout=(1, 2))","category":"page"},{"location":"examples/vector1/#Vector-Valued-Example","page":"Vector Valued Example","title":"Vector Valued Example","text":"","category":"section"},{"location":"examples/vector1/","page":"Vector Valued Example","title":"Vector Valued Example","text":"This example demonstarates learning a vector valued problem,","category":"page"},{"location":"examples/vector1/","page":"Vector Valued Example","title":"Vector Valued Example","text":"f(x) = beginpmatrixx_1 x_2x_1^2 - x_2^2endpmatrix","category":"page"},{"location":"examples/vector1/","page":"Vector Valued Example","title":"Vector Valued Example","text":"using Random\nusing Plots\nusing LinearAlgebra\nusing ARFF","category":"page"},{"location":"examples/vector1/","page":"Vector Valued Example","title":"Vector Valued Example","text":"# define components of vector valued function\nf1(x) = x[1]*x[2];\nf2(x) = x[1]^2 - x[2]^2;\n\nxx = LinRange(-2, 2, 101);\nyy = LinRange(-2, 2, 101);\n\nz1 = [f1([x_, y_]) for y_ in yy, x_ in xx];\nz2 = [f2([x_, y_]) for y_ in yy, x_ in xx];\n\np1 = contourf(xx, yy, z1)\nxlabel!(\"x\")\nylabel!(\"y\")\np2 = contourf(xx, yy, z2)\nxlabel!(\"x\")\nplot(p1, p2, layout=(1, 2), plot_title=\"Truth\")","category":"page"},{"location":"examples/vector1/#Generate-Training-Data","page":"Vector Valued Example","title":"Generate Training Data","text":"","category":"section"},{"location":"examples/vector1/","page":"Vector Valued Example","title":"Vector Valued Example","text":"First, we generate our training data","category":"page"},{"location":"examples/vector1/","page":"Vector Valued Example","title":"Vector Valued Example","text":"N = 10^3;\nd=2;\nRandom.seed!(100)\nx_data = [randn(2) for _ in 1:N];\ny_data = [[f1(x_), f2(x_)] for x_ in x_data];\ndata = DataSet(x_data, complex.(y_data));","category":"page"},{"location":"examples/vector1/#Initialize-Fourier-Model","page":"Vector Valued Example","title":"Initialize Fourier Model","text":"","category":"section"},{"location":"examples/vector1/","page":"Vector Valued Example","title":"Vector Valued Example","text":"Next, we need to initialize our FourierModel","category":"page"},{"location":"examples/vector1/","page":"Vector Valued Example","title":"Vector Valued Example","text":"K = 2^6;\nRandom.seed!(200)\nd = 2;\nF0 = FourierModel([1.0 * randn(d) for _ in 1:K], \n    [1.0 * randn(d) for _ in 1:K]);","category":"page"},{"location":"examples/vector1/#Set-Parameters-and-Train","page":"Vector Valued Example","title":"Set Parameters and Train","text":"","category":"section"},{"location":"examples/vector1/","page":"Vector Valued Example","title":"Vector Valued Example","text":"δ = 0.1; # rwm step size\nλ = 1e-6; # regularization\nn_epochs = 10^3; # total number of iterations\nn_ω_steps = 10; # number of steps between full β updates\nn_burn = n_epochs ÷ 10;\nγ = optimal_γ(d);\nω_max =Inf;\nadapt_covariance = true;\n\nΣ0 = Float64[1 0; 0 1];\nβ_solver! = (β, S, y, ω)-> solve_normal!(β, S, y, λ=λ);\n\nopts = ARFFOptions(n_epochs, n_ω_steps, δ, n_burn, γ, ω_max,adapt_covariance, \n    β_solver!, ARFF.mse_loss);\n\nRandom.seed!(1000);\nF = deepcopy(F0);\nΣ_mean, acceptance_rate, loss = train_rwm!(F, data, Σ0, opts, show_progress=false); nothing","category":"page"},{"location":"examples/vector1/#Evaluate-Results","page":"Vector Valued Example","title":"Evaluate Results","text":"","category":"section"},{"location":"examples/vector1/","page":"Vector Valued Example","title":"Vector Valued Example","text":"Looking at the trianing loss, we see the model appears to be well trained for the selected width, K:","category":"page"},{"location":"examples/vector1/","page":"Vector Valued Example","title":"Vector Valued Example","text":"plot(1:length(loss), loss, yscale=:log10, label=\"\")\nxlabel!(\"Epoch\")\nylabel!(\"Loss\")","category":"page"},{"location":"examples/vector1/","page":"Vector Valued Example","title":"Vector Valued Example","text":"Next, we look at how the components of the trained model look:","category":"page"},{"location":"examples/vector1/","page":"Vector Valued Example","title":"Vector Valued Example","text":"z1 = [real(F([x_, y_])[1]) for y_ in yy, x_ in xx];\nz2 = [real(F([x_, y_])[2]) for y_ in yy, x_ in xx];\n\np1 = contourf(xx, yy, z1)\nxlabel!(\"x\")\nylabel!(\"y\")\np2 = contourf(xx, yy, z2)\nxlabel!(\"x\")\nplot(p1, p2, layout=(1, 2),plot_title=\"Trained Model\")","category":"page"},{"location":"structs/activation/#Activation-Functions","page":"Activation Functions","title":"Activation Functions","text":"","category":"section"},{"location":"structs/activation/","page":"Activation Functions","title":"Activation Functions","text":"While the theory has been developed for the Fourier activation functions of type e^{i \\omega \\cdot x}, this module permits for general activation funcitons to be encoded in an ActivationFunction data structure.  ","category":"page"},{"location":"structs/activation/","page":"Activation Functions","title":"Activation Functions","text":"ActivationFunction","category":"page"},{"location":"structs/activation/#ARFF.ActivationFunction","page":"Activation Functions","title":"ARFF.ActivationFunction","text":"ActivationFunction{TB,TF} <: AbstractActivationFunction where {TB<:Number,TF<:Function}\n\nActivation function data structure.  TB should match the desired type of y from the data.\n\nFields\n\nσ - A scalar valued function that returns the TB data type.\n\n\n\n\n\n","category":"type"},{"location":"structs/activation/","page":"Activation Functions","title":"Activation Functions","text":"This structure encodes both the function, along with the return data type (real or complex) to ensure type consistency for performance.  The following example would define the ReLU activation function:","category":"page"},{"location":"structs/activation/","page":"Activation Functions","title":"Activation Functions","text":"using ARFF\nrelu(z) = z*(z>0);\nReLUActivation = ActivationFunction{Float64}(relu)","category":"page"},{"location":"structs/activation/","page":"Activation Functions","title":"Activation Functions","text":"This can then be passed in as an argument when defining a Fourier feature model.","category":"page"},{"location":"structs/activation/","page":"Activation Functions","title":"Activation Functions","text":"The ActivationFunction type can be evaluated:","category":"page"},{"location":"structs/activation/","page":"Activation Functions","title":"Activation Functions","text":"println(ReLUActivation([-1.], [1.]));\nprintln(ReLUActivation([1.], [1.]));","category":"page"},{"location":"structs/activation/","page":"Activation Functions","title":"Activation Functions","text":"NOTE It is essential that the data type, TN used when defining ActivationFunction{TN}(myactfunc) must be the same as the numerical type of y used in the construction of a data set.","category":"page"},{"location":"structs/activation/","page":"Activation Functions","title":"Activation Functions","text":"Some convenience functions are a part of the package:","category":"page"},{"location":"structs/activation/","page":"Activation Functions","title":"Activation Functions","text":"FourierActivation - The complex exponential for type ComplexF64\nSigmoidActivation - The sigmoid function for type Float64\nArcTanActivation - The arctan function for type Float64","category":"page"},{"location":"structs/data/#dataset","page":"Data Sets","title":"Data Sets","text":"","category":"section"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"Training (and testing data) are stored in a DataSet structure (with either scalar or vector valued y) ","category":"page"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"ARFF.ScalarDataSet\nARFF.VectorDataSet","category":"page"},{"location":"structs/data/#ARFF.ScalarDataSet","page":"Data Sets","title":"ARFF.ScalarDataSet","text":"ScalarDataSet{TC<:Complex,TR<:AbstractFloat,TW<:AbstractArray{TR},TI<:Integer}\n\nTraining data containing (x,y) data pairs stored in arrays of x values and arrays of y values.\n\nFields\n\nx - Array of real valued vectors \ny - Array of complex scalars\nN - Number of data points\ndx - Dimension of x coordinates\n\n\n\n\n\n","category":"type"},{"location":"structs/data/#ARFF.VectorDataSet","page":"Data Sets","title":"ARFF.VectorDataSet","text":"VectorDataSet{TC<:Complex,TR<:AbstractFloat,TW<:AbstractArray{TR},TB<:AbstractArray{TC},TI<:Integer}\n\nTraining data containing (x,y) data pairs stored in arrays of x values and arrays of y values.\n\nFields\n\nx - Array of real valued vectors \ny - Array of complex valued vectors\nyt - Tranposed array of complex valued vectors\nN - Number of data points\ndx - Dimension of x coordinates\ndy - Dimension of y coordinates\n\n\n\n\n\n","category":"type"},{"location":"structs/data/#Constructing-Data-Sets","page":"Data Sets","title":"Constructing Data Sets","text":"","category":"section"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"Data sets can be constructed by passing in the (x_iy_i) information:","category":"page"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"    DataSet","category":"page"},{"location":"structs/data/#ARFF.DataSet","page":"Data Sets","title":"ARFF.DataSet","text":"DataSet(x::Vector{Vector{TR}}, y::Vector{TY}) where {TR<:AbstractFloat,TY<:Number}\n\nConvenience constructor for a scalar valued data set\n\nFields\n\nx - Array of real valued vectors \ny - Array of scalars\n\n\n\n\n\nDataSet(x::Vector{Vector{TR}}, y::Vector{Vector{TY}}) where {TY<:Number, TR<:AbstractFloat}\n\nConvenience constructor for a vector valued data set\n\nFields\n\nx - Array of real valued vectors \ny - Array of vectors\n\n\n\n\n\n","category":"function"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"For instance, a scalar valued data set can be generated with: A training set can be constructed as follows:","category":"page"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"using ARFF\nusing Random \nRandom.seed!(100); # for reproducibility\n\nn = 10; # number of sample points\nx = [rand(2) for _ in 1:n];\nf(x) = exp(-x[1]*x[2]); # arbitrary function\ny = complex.(f.(x)); # make y valued data complex for type consistency\n\ndata = DataSet(x,y);\nprintln(data.x)\nprintln(data.y)","category":"page"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"Vector valued data can be similarly constructed:","category":"page"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"using ARFF\nusing Random \nRandom.seed!(100); # for reproducibility\n\nn = 10; # number of sample points\nx = [rand(2) for _ in 1:n];\nf(x) = [x[1]+x[2]; exp(-x[1]*x[2])]; # arbitrary function\ny = f.(x);\n\ndata = DataSet(x,y);\nprintln(data.x)\nprintln(data.y)","category":"page"},{"location":"structs/data/#scalings","page":"Data Sets","title":"Scaling Data Sets","text":"","category":"section"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"It is often helpful to scale the training data (setting means to zero and variances to unity).  These can be accomplished with the scalings structure and the associated functions.","category":"page"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"    get_scalings\n    scale_data!\n    rescale_data!","category":"page"},{"location":"structs/data/#ARFF.get_scalings","page":"Data Sets","title":"ARFF.get_scalings","text":"get_scalings(data::ScalarDataSet{TC,TR,TW,TI}) where {TC<:Complex,TR<:AbstractFloat,TW<:AbstractArray{TR},TI<:Integer}\n\nFind the means and variances of the data for scaling\n\nFields\n\ndata - The training data set\n\n\n\n\n\nget_scalings(data::VectorDataSet{TC,TR,TW,TB,TI}) where {TC<:Complex,TR<:AbstractFloat,TW<:AbstractArray{TR},TB<:AbstractArray{TC},TI<:Integer}\n\nFind the means and variances of the data for scaling\n\nFields\n\ndata - The training data set\n\n\n\n\n\n","category":"function"},{"location":"structs/data/#ARFF.scale_data!","page":"Data Sets","title":"ARFF.scale_data!","text":"scale_data!(data::ScalarDataSet{TC,TR,TW,TI}, scalings::ScalarDataScalings{TC,TR,TW}) where {TC<:Complex,TR<:AbstractFloat,TW<:AbstractArray{TR},TI<:Integer}\n\nScale the data set (in-place) according to the specified scalings\n\nFields\n\ndata - Data set to be scale\nscalings - Scalings to apply to data\n\n\n\n\n\nscale_data!(data::VectorDataSet{TC,TR,TW,TB,TI}, scalings::VectorDataScalings{TC,TR,TW,TB}) where {TC<:Complex,TR<:AbstractFloat,TW<:AbstractArray{TR}, TB<:AbstractArray{TC},TI<:Integer}\n\nScale the data set (in-place) according to the specified scalings\n\nFields\n\ndata - Data set to be scale\nscalings - Scalings to apply to data\n\n\n\n\n\n","category":"function"},{"location":"structs/data/#ARFF.rescale_data!","page":"Data Sets","title":"ARFF.rescale_data!","text":"rescale_data!(data::ScalarDataSet{TC,TR,TW}, scalings::ScalarDataScalings{TC,TR,TW}) where {TC<:Complex,TR<:AbstractFloat,TW<:AbstractArray{TR}}\n\nRescale the data set (in-place) according back to the original units\n\nFields\n\ndata - Data set to be scale\nscalings - Scalings to apply to data\n\n\n\n\n\nrescale_data!(data::VectorDataSet{TC,TR,TW,TB}, scalings::VectorDataScalings{TC,TR,TW,TB}) where {TC<:Complex,TR<:AbstractFloat,TW<:AbstractArray{TR}, TB<:AbstractArray{TC}}\n\nRescale the data set (in-place) according back to the original units\n\nFields\n\ndata - Data set to be scale\nscalings - Scalings to apply to data\n\n\n\n\n\n","category":"function"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"For example, the following code obtains the means and variances in the data:","category":"page"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"scalings = get_scalings(data);\nprintln(scalings.μx);\nprintln(scalings.σ2x);\nprintln(scalings.μy);\nprintln(scalings.σ2y);","category":"page"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"Next, we can scale our data:","category":"page"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"scale_data!(data, scalings);\n\nusing Statistics\nprintln(mean(data.x));\nprintln(var(data.x));\nprintln(mean(data.y));\nprintln(var(data.y));","category":"page"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"If needed, we can then undo the scaling,","category":"page"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"rescale_data!(data, scalings)\n\nprintln(mean(data.x));\nprintln(var(data.x));\nprintln(mean(data.y));\nprintln(var(data.y));","category":"page"},{"location":"structs/data/","page":"Data Sets","title":"Data Sets","text":"Note that these are in agreement with what was contained in our scalings structure.","category":"page"},{"location":"structs/fourier/#Fourier-Feature-Models","page":"Fourier Feature Models","title":"Fourier Feature Models","text":"","category":"section"},{"location":"structs/fourier/","page":"Fourier Feature Models","title":"Fourier Feature Models","text":"The generalized fourier features model approximates functions with K features, with acvtivation function varphi as","category":"page"},{"location":"structs/fourier/","page":"Fourier Feature Models","title":"Fourier Feature Models","text":"f_rm true(x) approx f(x) = sum_k=1^K beta_k varphi(xomega_k)","category":"page"},{"location":"structs/fourier/","page":"Fourier Feature Models","title":"Fourier Feature Models","text":"In the above expression, xomega_kin mathbbR^d, while beta_kinmathbbR^d or beta_kinmathbbC^d, depending on the choice of activation function.  Consequently, our model is uniquely determined by the coefficients and the wavenumbers.  ","category":"page"},{"location":"structs/fourier/","page":"Fourier Feature Models","title":"Fourier Feature Models","text":"These are stored in either a scalar or vector valued data structure:","category":"page"},{"location":"structs/fourier/","page":"Fourier Feature Models","title":"Fourier Feature Models","text":"ARFF.ScalarFourierModel\nARFF.VectorFourierModel","category":"page"},{"location":"structs/fourier/#ARFF.ScalarFourierModel","page":"Fourier Feature Models","title":"ARFF.ScalarFourierModel","text":"ScalarFourierModel{TB<:Complex,TW<:AbstractArray{AbstractFloat}}\n\nStructure containing a scalar valued fourier model which will be learned\n\nFields\n\nβ - Array of complex coefficients\nω - Array of wave vectors\nK - Number of Fourier features\ndx - Dimension of x coordinate\nϕ - Activation function\n\n\n\n\n\n","category":"type"},{"location":"structs/fourier/#ARFF.VectorFourierModel","page":"Fourier Feature Models","title":"ARFF.VectorFourierModel","text":"VectorFourierModel{TB<:Complex,TW<:AbstractArray{AbstractFloat}}\n\nStructure containing a scalar valued fourier model which will be learned\n\nFields\n\nβ - 2D array of coefficients; K by dy in size\nω - Array of wave vectors\nK - Number of Fourier features\ndx - Dimension of x coordinate\ndy - Dimension of y coordinate\n\n\n\n\n\n","category":"type"},{"location":"structs/fourier/#Constructing-a-Fourier-Features-Model","page":"Fourier Feature Models","title":"Constructing a Fourier Features Model","text":"","category":"section"},{"location":"structs/fourier/","page":"Fourier Feature Models","title":"Fourier Feature Models","text":"A Fourier features model can be instantiated with:","category":"page"},{"location":"structs/fourier/","page":"Fourier Feature Models","title":"Fourier Feature Models","text":"    FourierModel","category":"page"},{"location":"structs/fourier/#ARFF.FourierModel","page":"Fourier Feature Models","title":"ARFF.FourierModel","text":"FourierModel(β::Vector{TR}, ω::Vector{Vector{TR}}) where {TR<:AbstractFloat}\n\nTBW\n\n\n\n\n\nFourierModel(β::Vector{TB}, ω::Vector{Vector{TR}}, ϕ::TA) where {TB<:Number,TR<:AbstractFloat,TA<:ActivationFunction{TB}}\n\nTBW\n\n\n\n\n\nFourierModel(β::Vector{Vector{TR}}, ω::Vector{Vector{TR}}) where {TR<:AbstractFloat}\n\nTBW\n\n\n\n\n\nFourierModel(β::Vector{Vector{TB}}, ω::Vector{Vector{TR}}, ϕ::TA) where {TR<:AbstractFloat,TB<:Number,TA<:ActivationFunction{TB}}\n\nTBW\n\n\n\n\n\nFourierModel(β::Vector{Vector{TR}}, ω::Vector{Vector{TR}}) where {TR<:AbstractFloat}\n\nTBW\n\n\n\n\n\nFourierModel(β::Vector{Vector{TB}}, ω::Vector{Vector{TR}}, ϕ::TA) where {TR<:AbstractFloat,TB<:Number,TA<:ActivationFunction{TB}}\n\nTBW\n\n\n\n\n\n","category":"function"},{"location":"structs/fourier/","page":"Fourier Feature Models","title":"Fourier Feature Models","text":"As an example, the scalar valued Fourier model with complex exponentials can be instantiated as:","category":"page"},{"location":"structs/fourier/","page":"Fourier Feature Models","title":"Fourier Feature Models","text":"using ARFF\nusing Random\nRandom.seed!(200); # for reproducibility\n\nK = 10;\nd = 2;\n# convenience\nF = FourierModel(randn(ComplexF64, K),  [randn(d) for _ in 1:K]);","category":"page"},{"location":"structs/fourier/","page":"Fourier Feature Models","title":"Fourier Feature Models","text":"This defines F with random wavenumbers and amplitudes.  Strictly speaking, this is not required, but it is often helpful within the learning context that we will apply this method.  By not specifying an activation function, this defaults to FourierActivation function, e^{i \\omega \\cdot x}, and presumes all y related values are of the same complex type.","category":"page"},{"location":"structs/fourier/","page":"Fourier Feature Models","title":"Fourier Feature Models","text":"Function evaluation has been overloaded for a FourierModel, allowing us to evaluate it at points:","category":"page"},{"location":"structs/fourier/","page":"Fourier Feature Models","title":"Fourier Feature Models","text":"x_test = [0., 1.];\nprintln(F(x_test));","category":"page"},{"location":"structs/fourier/","page":"Fourier Feature Models","title":"Fourier Feature Models","text":"and it can perform a vectorized evaluation:","category":"page"},{"location":"structs/fourier/","page":"Fourier Feature Models","title":"Fourier Feature Models","text":"n = 10; # number of sample points\nx = [rand(2) for _ in 1:n];\nf(x) = exp(-x[1]*x[2]); # arbitrary function\ny = complex.(f.(x)); # make y valued data complex for type consistency\n\ndata = DataSet(x,y);\n\nF.(data.x);","category":"page"},{"location":"structs/fourier/","page":"Fourier Feature Models","title":"Fourier Feature Models","text":"As is sometimes the case, we may scale our data, train in the scale coordinate, and wish to evaluate new poitns in the original, unscaled, coordinate system. We can accomplish that by passing a DataScalings argument in for evaluation:","category":"page"},{"location":"structs/fourier/","page":"Fourier Feature Models","title":"Fourier Feature Models","text":"scalings = get_scalings(data);\n\nF(x_test, scalings)","category":"page"},{"location":"structs/fourier/","page":"Fourier Feature Models","title":"Fourier Feature Models","text":"If one wishes to use a different activation function, we pass that as an argument in the construction:","category":"page"},{"location":"structs/fourier/","page":"Fourier Feature Models","title":"Fourier Feature Models","text":"G = FourierModel(randn(K),  [randn(d) for _ in 1:K],SigmoidActivation);\nprintln(G([0.1, 0.2]))","category":"page"},{"location":"aux/#Auxiliary-Functions-and-Utilities","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"","category":"section"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"Pages = [\"aux.md\"]","category":"page"},{"location":"aux/#linalg","page":"Auxiliary Functions and Utilities","title":"Linear Algebra","text":"","category":"section"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"It is essential to be able solve for the updated boldsymbolbeta when we update the boldsymbolomega.  In a typical setting, this corresponds to solving","category":"page"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"(S^astS + N lambda I)boldsymbolbeta = S^ast boldsymboly","category":"page"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"We have included two naive solvers for this problem:","category":"page"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"solve_normal!\nsolve_normal_svd!","category":"page"},{"location":"aux/#ARFF.solve_normal!","page":"Auxiliary Functions and Utilities","title":"ARFF.solve_normal!","text":"solve_normal!(β, S, y_data; λ = 1e-8)\n\nSolve the regularized linear system using the normal equations.\n\nFields\n\nβ - The vector of coefficients that will be obtained\nS - The design matrix\ny_data - y coordinates\nλ = 1e-8 - Regularization parameter\n\n\n\n\n\n","category":"function"},{"location":"aux/#ARFF.solve_normal_svd!","page":"Auxiliary Functions and Utilities","title":"ARFF.solve_normal_svd!","text":"solve_normal_svd!(β, S, y_data; λ = 1e-8)\n\nSolve the regularized linear system using the SVD.\n\nFields\n\nβ - The vector of coefficients that will be obtained\nS - The design matrix\ny_data - y coordinates\nλ = 1e-8 - Regularization parameter\n\n\n\n\n\n","category":"function"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"Other formulations may be more appropriate.  Indeed, in [2], the authors use the regularized loss function in the spirit of Sobolev:","category":"page"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"Sboldsymbolbeta-boldsymboly_2^2 + lambda sum_k (1+omega_k^2)beta_k^2","category":"page"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"We thus require that the user provided linear solver take the form:","category":"page"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"function linear_solver!(β, S, y, ω)\n    # solve for β coefficients\n    β\nend","category":"page"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"Obviously, if one does not need ω for your formulation, as is the case in the original regularization of [1], this argument is just ignored.  For solve_normal! we would implement this as:","category":"page"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"linear_solver! = (β, S, y, ω) -> solve_normal!(β, S, y)","category":"page"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"In the case that one is solving a vector valued problem, the vector valued beta's are obtained component by component against the vector valued y's. ","category":"page"},{"location":"aux/#loss","page":"Auxiliary Functions and Utilities","title":"Loss Functions","text":"","category":"section"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"ARFF.mse_loss","category":"page"},{"location":"aux/#ARFF.mse_loss","page":"Auxiliary Functions and Utilities","title":"ARFF.mse_loss","text":"mse_loss(F::ScalarFourierModel{TC,TR,TW}, data_x::Vector{TW}, data_y::Vector{TC}) where {TC<:Complex,TR<:AbstractFloat,TW<:AbstractArray{TR}}\n\nMean squared error loss function\n\nFields\n\nF - A FourierModel structure\ndata_x - the x coordinates in training data\ndata_y - the y coordinates in training data\n\n\n\n\n\nmse_loss(F::VectorFourierModel{TC,TR,TW,TB}, data_x::Vector{TW}, data_y::Vector{TB}) where {TC<:Complex,TR<:AbstractFloat,TW<:AbstractArray{TR}, TB<:AbstractArray{TC}}\n\nMean squared error loss function\n\nFields\n\nF - A FourierModel structure\ndata_x - the x coordinates in training data\ndata_y - the y coordinates in training data\n\n\n\n\n\n","category":"function"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"As the entire framework is built around the mean square loss function, we have included it for convenience.  Other loss functions can be implemented, but they should have the calling sequence:","category":"page"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"function loss_function(F, data_x, data_y)\n    # compute loss \n    return loss\nend","category":"page"},{"location":"aux/#bias","page":"Auxiliary Functions and Utilities","title":"Adding Bias","text":"","category":"section"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"It may be neccessary to modify an existing data set so as to include a constant bias term in a model.  If xin mathbbR^d, then ","category":"page"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"xmapsto (x1)=tildexin mathbbR^d+1","category":"page"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"This is relevant when using generalized activation functions; see Scalar Example with Generalized Activation Functions.  We provide tools for account for the constant in both the DataSet and DataScalings types:","category":"page"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"append_bias(data::ARFF.ScalarDataSet{TR,TY,TI}) where {TY<:Number,TR<:AbstractFloat,TI<:Integer}\nappend_bias(scalings::ARFF.ScalarDataScalings{TR,TY}) where {TY<:Number,TR<:AbstractFloat}","category":"page"},{"location":"aux/#ARFF.append_bias-Union{Tuple{ARFF.ScalarDataSet{TR, TY, TI}}, Tuple{TI}, Tuple{TR}, Tuple{TY}} where {TY<:Number, TR<:AbstractFloat, TI<:Integer}","page":"Auxiliary Functions and Utilities","title":"ARFF.append_bias","text":"append_bias(data)\n\nCreate a new DataSet with a bias term appended to the end of the x coordinate.\n\nFields\n\ndata - data set to be augmented\n\n\n\n\n\n","category":"method"},{"location":"aux/#ARFF.append_bias-Union{Tuple{ARFF.ScalarDataScalings{TR, TY}}, Tuple{TR}, Tuple{TY}} where {TY<:Number, TR<:AbstractFloat}","page":"Auxiliary Functions and Utilities","title":"ARFF.append_bias","text":"append_bias(scalings)\n\nCreate a new DataScalings accounting for a bias term appended to the end of the x coordinate.\n\nFields\n\nscalings - scalings to be augmented\n\n\n\n\n\n","category":"method"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"In this framework, we would compute and/or apply the scalings before adding the bias term to the data set, and then add the bias in to both the data and the scaling structures.  This avoids a potential divide by zero issue.","category":"page"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"using ARFF\nusing Statistics\nusing Printf","category":"page"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"n_x = 10\nx = [Float64[i] for i in 1:n_x]\nf(x) = x[1]^2\ny = f.(x)\ndata_ = DataSet(x, y)\nscalings_ = get_scalings(data_);\ndata_scaled = scale_data!(data_, scalings_);","category":"page"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"The means and variances of the scaled data set are as we would expect:","category":"page"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"println(mean(data_scaled.x));\nprintln(var(data_scaled.x));","category":"page"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"and the scalings_ has the relevant information:","category":"page"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"println(scalings_.μx);\nprintln(scalings_.σ2x);","category":"page"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"Next, we can add in the bias into the data set and check that it is computing properly:","category":"page"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"data_bias = append_bias(data_scaled);\nprintln(mean(data_bias.x));\nprintln(var(data_bias.x));","category":"page"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"and analogously with the scalings:","category":"page"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"scalings_bias = append_bias(scalings_);\nprintln(scalings_bias.μx);\nprintln(scalings_bias.σ2x);","category":"page"},{"location":"aux/#Other-Utilities","page":"Auxiliary Functions and Utilities","title":"Other Utilities","text":"","category":"section"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"optimal_γ","category":"page"},{"location":"aux/#ARFF.optimal_γ","page":"Auxiliary Functions and Utilities","title":"ARFF.optimal_γ","text":"optimal_γ(d)\n\nCompute the optimal γ parameter as a function of dimension d\n\nFields\n\nd - the dimension of the x coordinate\n\n\n\n\n\n","category":"function"},{"location":"aux/","page":"Auxiliary Functions and Utilities","title":"Auxiliary Functions and Utilities","text":"Following Remark 1 in [1], the optimal gamma corresponds to gamma = 3d -2, which is encoded in the above function.","category":"page"},{"location":"examples/example1/#Scalar-Example","page":"Scalar Example","title":"Scalar Example","text":"","category":"section"},{"location":"examples/example1/","page":"Scalar Example","title":"Scalar Example","text":"This example is taken from Section 5 of [1]. Consider trying to learn the scalar mapping","category":"page"},{"location":"examples/example1/","page":"Scalar Example","title":"Scalar Example","text":"f(x) = mathrmSileft(fracxaright)e^-fracx^22 quad a0","category":"page"},{"location":"examples/example1/","page":"Scalar Example","title":"Scalar Example","text":"where mathrmSi is the Sine integral.  To make the problem somewhat challenging, we take a=10^-3.","category":"page"},{"location":"examples/example1/#Generate-Training-Data","page":"Scalar Example","title":"Generate Training Data","text":"","category":"section"},{"location":"examples/example1/","page":"Scalar Example","title":"Scalar Example","text":"First, we will generate and visualize the training data:","category":"page"},{"location":"examples/example1/","page":"Scalar Example","title":"Scalar Example","text":"using SpecialFunctions\nusing Random\nusing Plots\nusing LinearAlgebra\nusing ARFF\n\na = 1e-3;\nf(x) = sinint(x/a) * exp(-0.5 * (x^2));\n\nn_x = 500; # number of training points\nd = 1;\nRandom.seed!(100); # for reproducibility\nx = [0.1*rand(d) for _ in 1:n_x];\ny = [f(x_[1]) for x_ in x];\n\n# store data in DataSet structure\ndata = DataSet(x,complex.(y));\n\nscatter([x_[1] for x_ in x], y, label=\"Sample Points\")\nxx = LinRange(0, 0.1, 500);\nplot!(xx, f.(xx), label=\"f(x)\")\nxlabel!(\"x\")","category":"page"},{"location":"examples/example1/","page":"Scalar Example","title":"Scalar Example","text":"Note: When the domain of our target function is mathbbR^1, the x-data must still be stored as an array of arrays of length one, not an array of scalars.  We also do not require DataScalings for this problem.","category":"page"},{"location":"examples/example1/#Initialize-Fourier-Model","page":"Scalar Example","title":"Initialize Fourier Model","text":"","category":"section"},{"location":"examples/example1/","page":"Scalar Example","title":"Scalar Example","text":"Next, we need to initialize our FourierModel","category":"page"},{"location":"examples/example1/","page":"Scalar Example","title":"Scalar Example","text":"K = 2^7;\nRandom.seed!(200); # for reproducibility\nF0 = FourierModel([1. *randn(ComplexF64) for _ in 1:K],  \n    [randn(d) for _ in 1:K]); nothing","category":"page"},{"location":"examples/example1/","page":"Scalar Example","title":"Scalar Example","text":"This uses the default complex exponential activation functions.","category":"page"},{"location":"examples/example1/#Set-Parameters-and-Train","page":"Scalar Example","title":"Set Parameters and Train","text":"","category":"section"},{"location":"examples/example1/","page":"Scalar Example","title":"Scalar Example","text":"δ = 10.; # rwm step size\nλ = 1e-8; # regularization\nn_epochs = 10^3; # number of epochs\nn_ω_steps = 10; # number of steps between full β updates\nn_burn = n_epochs ÷ 10; # use 10% of the run for burn in\nγ = optimal_γ(d); \nω_max =Inf; # no cut off\nadapt_covariance = true; \n\nΣ0 = ones(1,1);\n\nβ_solver! = (β, S, y, ω)-> solve_normal!(β, S, y, λ=λ);\n\nopts = ARFFOptions(n_epochs, n_ω_steps, δ, n_burn, γ, ω_max,adapt_covariance, \n    β_solver!, ARFF.mse_loss);\n\nRandom.seed!(1000); # for reproducibility\nF = deepcopy(F0);\nΣ_mean, acceptance_rate, loss= train_rwm!(F, data, Σ0, opts, show_progress=false); nothing ","category":"page"},{"location":"examples/example1/#Evaluate-Results","page":"Scalar Example","title":"Evaluate Results","text":"","category":"section"},{"location":"examples/example1/","page":"Scalar Example","title":"Scalar Example","text":"Looking at the trianing loss, we see the model appears to be well trained for the selected width, K:","category":"page"},{"location":"examples/example1/","page":"Scalar Example","title":"Scalar Example","text":"plot(1:length(loss), loss, yscale=:log10, label=\"\")\nxlabel!(\"Epoch\")\nylabel!(\"Loss\")","category":"page"},{"location":"examples/example1/","page":"Scalar Example","title":"Scalar Example","text":"Next, we can verify that we have a high quality approximation of the true f(x):","category":"page"},{"location":"examples/example1/","page":"Scalar Example","title":"Scalar Example","text":"xx = LinRange(0, .1, 500);\nscatter([x_[1] for x_ in x], y, label=\"Sample Points\", legend=:right)\nplot!(xx, f.(xx), label = \"Truth\" )\nplot!(xx, real.([F([x_]) for x_ in xx]),label=\"Learned Model (Real Part)\")\nplot!(xx, imag.([F([x_]) for x_ in xx]),label=\"Learned Model (Imaginary Part)\" )\nxlabel!(\"x\")","category":"page"},{"location":"examples/example1/","page":"Scalar Example","title":"Scalar Example","text":"We can also verify that the training data is well fit:","category":"page"},{"location":"examples/example1/","page":"Scalar Example","title":"Scalar Example","text":"scatter(real.(data.y),real.(F.(data.x)),label=\"Training Data\")\nxx = LinRange(0,2,100);\nplot!(xx, xx, ls=:dash, label=\"\")\nxlabel!(\"Truth\")\nylabel!(\"Prediction\")","category":"page"},{"location":"train/#Training","page":"Training","title":"Training","text":"","category":"section"},{"location":"train/","page":"Training","title":"Training","text":"Pages = [\"train.md\"]","category":"page"},{"location":"train/","page":"Training","title":"Training","text":"The key function is train_rwm!, which performs in place training on the model. This is implemented to handle the training data in several ways:","category":"page"},{"location":"train/","page":"Training","title":"Training","text":"A single DataSet can be provided and used in every training epoch.\nA single DataSet and a minibatch size can be provided, and minibatchs will be generated at each epoch.\nAn array of DataSet types can be provided, and they will be cycled through each epoch;","category":"page"},{"location":"train/#In-Place-Training","page":"Training","title":"In Place Training","text":"","category":"section"},{"location":"train/","page":"Training","title":"Training","text":"For both the scalar and vector valued case, we have the following commands:","category":"page"},{"location":"train/","page":"Training","title":"Training","text":"    train_rwm!(F::ARFF.ScalarFourierModel{TR,TB,TI,TA}, data::ARFF.ScalarDataSet{TR,TB,TI}, Σ::Matrix{TR}, options::ARFF.ARFFOptions; show_progress=true, record_loss=true) where {TB<:Number,TR<:AbstractFloat,TI<:Integer, TA<:ARFF.ActivationFunction{TB}}\n    train_rwm!(F::ARFF.ScalarFourierModel{TR,TB,TI,TA}, data::ARFF.ScalarDataSet{TR,TB,TI}, batch_size::TI, Σ::Matrix{TR}, options::ARFF.ARFFOptions; show_progress=true, record_loss=true) where {TB<:Number,TR<:AbstractFloat,TI<:Integer,TA<:ARFF.ActivationFunction{TB}}    \n    train_rwm!(F::ARFF.ScalarFourierModel{TR,TB,TI,TA}, data_sets::Vector{ARFF.ScalarDataSet{TR,TB,TI}}, Σ::Matrix{TR}, options::ARFF.ARFFOptions; show_progress=true, record_loss=true) where {TB<:Number,TR<:AbstractFloat,TI<:Integer,TA<:ARFF.ActivationFunction{TB}}","category":"page"},{"location":"train/#ARFF.train_rwm!-Union{Tuple{TA}, Tuple{TI}, Tuple{TR}, Tuple{TB}, Tuple{ARFF.ScalarFourierModel{TR, TB, TI, TA}, ARFF.ScalarDataSet{TR, TB, TI}, Matrix{TR}, ARFFOptions}} where {TB<:Number, TR<:AbstractFloat, TI<:Integer, TA<:(ActivationFunction{TB})}","page":"Training","title":"ARFF.train_rwm!","text":"train_rwm!(F, data, Σ, options; show_progress=true, record_loss=true)\n\nF - The FourierModel to be trained\ndata- A DataSet training data set\nΣ - Initial covariance matrix for RWM proposals\noptions - ARFFOptions structure specifcying the number epochs, proposal step size, etc.\nshow_progress=true - Display training progress using ProgressMeter\nrecord_loss=true - Evaluate the specified loss function at each epoch and record\n\n\n\n\n\n","category":"method"},{"location":"train/#ARFF.train_rwm!-Union{Tuple{TA}, Tuple{TI}, Tuple{TR}, Tuple{TB}, Tuple{ARFF.ScalarFourierModel{TR, TB, TI, TA}, ARFF.ScalarDataSet{TR, TB, TI}, TI, Matrix{TR}, ARFFOptions}} where {TB<:Number, TR<:AbstractFloat, TI<:Integer, TA<:(ActivationFunction{TB})}","page":"Training","title":"ARFF.train_rwm!","text":"train_rwm!(F, data, batch_size, Σ, options; show_progress=true, record_loss=true)\n\nF - The FourierModel to be trained\ndata- A DataSet training data set\nbatch_size - Minibatch size\nΣ - Initial covariance matrix for RWM proposals\noptions - ARFFOptions structure specifcying the number epochs, proposal step size, etc.\nshow_progress=true - Display training progress using ProgressMeter\nrecord_loss=true - Evaluate the specified loss function at each epoch and record\n\n\n\n\n\n","category":"method"},{"location":"train/#ARFF.train_rwm!-Union{Tuple{TA}, Tuple{TI}, Tuple{TR}, Tuple{TB}, Tuple{ARFF.ScalarFourierModel{TR, TB, TI, TA}, Array{ARFF.ScalarDataSet{TR, TB, TI}, 1}, Matrix{TR}, ARFFOptions}} where {TB<:Number, TR<:AbstractFloat, TI<:Integer, TA<:(ActivationFunction{TB})}","page":"Training","title":"ARFF.train_rwm!","text":"train_rwm!(F, data_sets, Σ, options; show_progress=true, record_loss=true)\n\nF - The FourierModel to be trained\ndata_sets- A vector of DataSet training data sets.  These are presumed to all be the same size.  They will be cycled through each epoch.\nΣ - Initial covariance matrix for RWM proposals\noptions - ARFFOptions structure specifcying the number epochs, proposal step size, etc.\nshow_progress=true - Display training progress using ProgressMeter\nrecord_loss=true - Evaluate the specified loss function at each epoch and record\n\n\n\n\n\n","category":"method"},{"location":"train/","page":"Training","title":"Training","text":"Having created an initial F we can then call","category":"page"},{"location":"train/","page":"Training","title":"Training","text":"Σ_mean, acceptance_rate, loss = train_rwm!(F, data, Σ0, opts);","category":"page"},{"location":"train/","page":"Training","title":"Training","text":"The returned quantities are the mean adapted covariance matrix Σ_mean.  The acceptance_rate is the mean acceptance rate at each epoch, averaged overa the internal steps,  K * n_ω_steps.  The loss is the recorded training loss, stored in opts.loss, at each epoch.","category":"page"},{"location":"train/#Recording-the-Training-Trajectory","page":"Training","title":"Recording the Training Trajectory","text":"","category":"section"},{"location":"train/","page":"Training","title":"Training","text":"We have also included routines which record the model at each epoch.  These are called in the same way as above:","category":"page"},{"location":"train/","page":"Training","title":"Training","text":"F_trajectory, Σ_mean, acceptance_rate, loss = train_rwm(F0, data, Σ0, opts);","category":"page"},{"location":"train/","page":"Training","title":"Training","text":"The functions are analogously named:","category":"page"},{"location":"train/","page":"Training","title":"Training","text":"    train_rwm(F::ARFF.ScalarFourierModel{TR,TB,TI,TA}, data::ARFF.ScalarDataSet{TR,TB,TI}, Σ::Matrix{TR}, options::ARFF.ARFFOptions; show_progress=true, record_loss=true) where {TB<:Number,TR<:AbstractFloat,TI<:Integer, TA<:ARFF.ActivationFunction{TB}}\n    train_rwm(F::ARFF.ScalarFourierModel{TR,TB,TI,TA}, data::ARFF.ScalarDataSet{TR,TB,TI}, batch_size::TI, Σ::Matrix{TR}, options::ARFF.ARFFOptions; show_progress=true, record_loss=true) where {TB<:Number,TR<:AbstractFloat,TI<:Integer,TA<:ARFF.ActivationFunction{TB}}    \n    train_rwm(F::ARFF.ScalarFourierModel{TR,TB,TI,TA}, data_sets::Vector{ARFF.ScalarDataSet{TR,TB,TI}}, Σ::Matrix{TR}, options::ARFF.ARFFOptions; show_progress=true, record_loss=true) where {TB<:Number,TR<:AbstractFloat,TI<:Integer,TA<:ARFF.ActivationFunction{TB}}","category":"page"},{"location":"train/#ARFF.train_rwm-Union{Tuple{TA}, Tuple{TI}, Tuple{TR}, Tuple{TB}, Tuple{ARFF.ScalarFourierModel{TR, TB, TI, TA}, ARFF.ScalarDataSet{TR, TB, TI}, Matrix{TR}, ARFFOptions}} where {TB<:Number, TR<:AbstractFloat, TI<:Integer, TA<:(ActivationFunction{TB})}","page":"Training","title":"ARFF.train_rwm","text":"train_rwm(F₀, data, Σ, options; show_progress=true, record_loss=true)\n\nF₀ - The initial state of the FourierModel to be trained\ndata- A DataSet training data set\nΣ - Initial covariance matrix for RWM proposals\noptions - ARFFOptions structure specifcying the number epochs, proposal step size, etc.\nshow_progress=true - Display training progress using ProgressMeter\nrecord_loss=true - Evaluate the specified loss function at each epoch and record\n\n\n\n\n\n","category":"method"},{"location":"train/#ARFF.train_rwm-Union{Tuple{TA}, Tuple{TI}, Tuple{TR}, Tuple{TB}, Tuple{ARFF.ScalarFourierModel{TR, TB, TI, TA}, ARFF.ScalarDataSet{TR, TB, TI}, TI, Matrix{TR}, ARFFOptions}} where {TB<:Number, TR<:AbstractFloat, TI<:Integer, TA<:(ActivationFunction{TB})}","page":"Training","title":"ARFF.train_rwm","text":"train_rwm(F₀, data, batch_size, Σ, options; show_progress=true, record_loss=true)\n\nF₀ - The initial state of the FourierModel to be trained\ndata- A DataSet training data set\nbatch_size - Minibatch size\nΣ - Initial covariance matrix for RWM proposals\noptions - ARFFOptions structure specifcying the number epochs, proposal step size, etc.\nshow_progress=true - Display training progress using ProgressMeter\nrecord_loss=true - Evaluate the specified loss function at each epoch and record\n\n\n\n\n\n","category":"method"},{"location":"train/#ARFF.train_rwm-Union{Tuple{TA}, Tuple{TI}, Tuple{TR}, Tuple{TB}, Tuple{ARFF.ScalarFourierModel{TR, TB, TI, TA}, Array{ARFF.ScalarDataSet{TR, TB, TI}, 1}, Matrix{TR}, ARFFOptions}} where {TB<:Number, TR<:AbstractFloat, TI<:Integer, TA<:(ActivationFunction{TB})}","page":"Training","title":"ARFF.train_rwm","text":"train_rwm(F₀, data_sets, Σ, options; show_progress=true, record_loss=true)\n\nF₀ - The initial state of the FourierModel to be trained\ndata_sets- A vector of DataSet training data sets.  These are presumed to all be the same size.  They will be cycled through each epoch.\nΣ - Initial covariance matrix for RWM proposals\noptions - ARFFOptions structure specifcying the number epochs, proposal step size, etc.\nshow_progress=true - Display training progress using ProgressMeter\nrecord_loss=true - Evaluate the specified loss function at each epoch and record\n\n\n\n\n\n","category":"method"},{"location":"train/","page":"Training","title":"Training","text":"This records the result at the end of each epoch.","category":"page"},{"location":"examples/example2/#Scalar-Example-with-Generalized-Activation-Functions","page":"Scalar Example with Generalized Activation Functions","title":"Scalar Example with Generalized Activation Functions","text":"","category":"section"},{"location":"examples/example2/","page":"Scalar Example with Generalized Activation Functions","title":"Scalar Example with Generalized Activation Functions","text":"This example demonstates learning with generalized activation functions.  For this problem, ","category":"page"},{"location":"examples/example2/","page":"Scalar Example with Generalized Activation Functions","title":"Scalar Example with Generalized Activation Functions","text":"f(x) = e^-x^22","category":"page"},{"location":"examples/example2/","page":"Scalar Example with Generalized Activation Functions","title":"Scalar Example with Generalized Activation Functions","text":"and we will work with sigmoid activation functions.  For this problem, we will augment our data to allow for a bias term, training, with an abuse of notation,","category":"page"},{"location":"examples/example2/","page":"Scalar Example with Generalized Activation Functions","title":"Scalar Example with Generalized Activation Functions","text":"f(x) approx F(tildex=(x1)) = sum_k beta_k varphi(tildexomega_k)","category":"page"},{"location":"examples/example2/","page":"Scalar Example with Generalized Activation Functions","title":"Scalar Example with Generalized Activation Functions","text":"The tildex is the variable padded with a unit value.  Thus, the problem will be studied as though it were over mathbbR^2.","category":"page"},{"location":"examples/example2/#Generate-Training-Data","page":"Scalar Example with Generalized Activation Functions","title":"Generate Training Data","text":"","category":"section"},{"location":"examples/example2/","page":"Scalar Example with Generalized Activation Functions","title":"Scalar Example with Generalized Activation Functions","text":"using Random\nusing Plots\nusing LinearAlgebra\nusing ARFF","category":"page"},{"location":"examples/example2/","page":"Scalar Example with Generalized Activation Functions","title":"Scalar Example with Generalized Activation Functions","text":"First, we will generate and visualize the training data:","category":"page"},{"location":"examples/example2/","page":"Scalar Example with Generalized Activation Functions","title":"Scalar Example with Generalized Activation Functions","text":"f(x) = exp(-0.5 * (x^2));\n\nn_x = 100; # number of training points\nd = 2;\nRandom.seed!(100); # for reproducibility\n# generate n_x sample points\nx = [[4*rand()] for _ in 1:n_x]; \ny = [f(x_[1]) for x_ in x];\n\n# store data in DataSet structure\ndata_ = DataSet(x,y);\n# append the bias term to our data\ndata = append_bias(data_);\n\nscatter([x_[1] for x_ in x], y, label=\"Sample Points\")\nxx = LinRange(0, 4, 100);\nplot!(xx, f.(xx), label=\"f(x)\")\nxlabel!(\"x\")","category":"page"},{"location":"examples/example2/#Initialize-Fourier-Model-with-Generalized-Features","page":"Scalar Example with Generalized Activation Functions","title":"Initialize Fourier Model with Generalized Features","text":"","category":"section"},{"location":"examples/example2/","page":"Scalar Example with Generalized Activation Functions","title":"Scalar Example with Generalized Activation Functions","text":"Next, we need to initialize our FourierModel","category":"page"},{"location":"examples/example2/","page":"Scalar Example with Generalized Activation Functions","title":"Scalar Example with Generalized Activation Functions","text":"K = 2^7;\nRandom.seed!(200); # for reproducibility\nF0 = FourierModel([1. *randn() for _ in 1:K],  \n    [randn(d) for _ in 1:K],SigmoidActivation); nothing","category":"page"},{"location":"examples/example2/#Set-Parameters-and-Train","page":"Scalar Example with Generalized Activation Functions","title":"Set Parameters and Train","text":"","category":"section"},{"location":"examples/example2/","page":"Scalar Example with Generalized Activation Functions","title":"Scalar Example with Generalized Activation Functions","text":"δ = 0.1; # rwm step size\nλ = 1e-6; # regularization\nn_epochs = 10^3; # total number of iterations\nn_ω_steps = 10; # number of steps between full β updates\nn_burn = n_epochs ÷ 10;\nγ = optimal_γ(d);\nω_max =Inf;\nadapt_covariance = true;\n\nΣ0 = Float64[1 0; 0 1];\n\n\nβ_solver! = (β, S, y, ω)-> solve_normal!(β, S, y, λ=λ);\n\nopts = ARFFOptions(n_epochs, n_ω_steps, δ, n_burn, γ, ω_max,adapt_covariance, \n    β_solver!, ARFF.mse_loss);\n\nRandom.seed!(1000); # for reproducibility\nF = deepcopy(F0);\nΣ_mean, acceptance_rate, loss= train_rwm!(F, data, Σ0, opts, show_progress=false); nothing ","category":"page"},{"location":"examples/example2/#Evaluate-Results","page":"Scalar Example with Generalized Activation Functions","title":"Evaluate Results","text":"","category":"section"},{"location":"examples/example2/","page":"Scalar Example with Generalized Activation Functions","title":"Scalar Example with Generalized Activation Functions","text":"Looking at the trianing loss, we see the model appears to be well trained for the selected width, K:","category":"page"},{"location":"examples/example2/","page":"Scalar Example with Generalized Activation Functions","title":"Scalar Example with Generalized Activation Functions","text":"plot(1:length(loss), loss, yscale=:log10, label=\"\")\nxlabel!(\"Epoch\")\nylabel!(\"Loss\")","category":"page"},{"location":"examples/example2/","page":"Scalar Example with Generalized Activation Functions","title":"Scalar Example with Generalized Activation Functions","text":"Next, we can verify that we have a high quality approximation of the true f(x):","category":"page"},{"location":"examples/example2/","page":"Scalar Example with Generalized Activation Functions","title":"Scalar Example with Generalized Activation Functions","text":"xx = LinRange(0, 4, 500);\nscatter([x_[1] for x_ in x], y, label=\"Sample Points\", legend=:right)\nplot!(xx, f.(xx), label = \"Truth\" )\nplot!(xx, [F([x_, 1]) for x_ in xx],label=\"Learned Model\")\nxlabel!(\"x\")","category":"page"},{"location":"examples/example2/","page":"Scalar Example with Generalized Activation Functions","title":"Scalar Example with Generalized Activation Functions","text":"We can also verify that the training data is well fit:","category":"page"},{"location":"examples/example2/","page":"Scalar Example with Generalized Activation Functions","title":"Scalar Example with Generalized Activation Functions","text":"scatter(data.y,F.(data.x),label=\"Data\")\nxx = LinRange(0,1,100);\nplot!(xx, xx, ls=:dash, label=\"\")\nxlabel!(\"Truth\")\nylabel!(\"Prediction\")","category":"page"},{"location":"#ARFF.jl-Documentation","page":"Home","title":"ARFF.jl Documentation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for the adaptive random fourier features (ARFF) package.  This package is built around the methodology presented in [1].","category":"page"},{"location":"","page":"Home","title":"Home","text":"Using the package involves three steps:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Formatting your training data into a DataSet structure\nInitializing a [FourierModel]  structure\nTraining","category":"page"},{"location":"#Overview","page":"Home","title":"Overview","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The essential idea of ARFF is to make an approximation of a true function, f_rm truemathbbR^dto mathbbC, as","category":"page"},{"location":"","page":"Home","title":"Home","text":"f_rm true(x) approx f(x) = sum_k=1^K beta_k e^i omega_k cdot x","category":"page"},{"location":"","page":"Home","title":"Home","text":"where xomega_kin mathbbR^d, while beta_kinmathbbC.  In the naive random Fourier featuer setting, the omega_k are sampled from some known distribution mu, and the beta_k are obtained by classical least squares regression or ridge regression,","category":"page"},{"location":"","page":"Home","title":"Home","text":"(S^astS + N lambda I)boldsymbolbeta = S^ast boldsymboly","category":"page"},{"location":"","page":"Home","title":"Home","text":"where the design matrix, S, is Ntimes K, with entries","category":"page"},{"location":"","page":"Home","title":"Home","text":"S_jk = e^ i omega_k cdot x_j","category":"page"},{"location":"","page":"Home","title":"Home","text":"We presume that we have training data of size N, (x_jy_j)_j=1^N. Other solutions are possible.","category":"page"},{"location":"","page":"Home","title":"Home","text":"This package generalizes the method to allow for both vector valued functions and permit activation functions other than the complex exponential:","category":"page"},{"location":"","page":"Home","title":"Home","text":"f(x) = sum_k=1^K beta_k varphi(xomega_k)","category":"page"},{"location":"","page":"Home","title":"Home","text":"where now beta_k in mathbbR^d or beta_k in mathbbC^d, where d need not be the same as d.","category":"page"},{"location":"#Adaptivity","page":"Home","title":"Adaptivity","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To make the algorithm adaptive, that is to say, to sample the frequencies from an optimal distribution, we use a Random Walk Metropolis scheme described in [1].  The goal is to sample from the variance minimizing distribution, known to be propto hatf(omega).","category":"page"},{"location":"","page":"Home","title":"Home","text":"The strategy is as follows:","category":"page"},{"location":"#Generate-Proposal","page":"Home","title":"Generate Proposal","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Perturb the vector boldsymbolomega of wave numbers with a Gaussian,","category":"page"},{"location":"","page":"Home","title":"Home","text":"boldsymbolomega = boldsymbolomega + delta boldsymbolxi quad boldsymbolxisim N(0 Sigma)","category":"page"},{"location":"","page":"Home","title":"Home","text":"where delta0 is a proposal step size and  Sigma is a covariance matrix. ","category":"page"},{"location":"#Update-Amplitudes","page":"Home","title":"Update Amplitudes","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Compute the proposed amplitudes, boldsymbolbeta for the perturbed wave numbers, by building up the new design matrix and solving the linear system. ","category":"page"},{"location":"#Accept/Reject","page":"Home","title":"Accept/Reject","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Accept/reject each wave vector omega_k with probability","category":"page"},{"location":"","page":"Home","title":"Home","text":"minleft1 fracbeta_k^gammabeta_k^gammaright","category":"page"},{"location":"","page":"Home","title":"Home","text":"where gamma0 is a tuning parameter that plays a role anlogous to inverse temperature.","category":"page"},{"location":"#Training","page":"Home","title":"Training","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Having described a single the RWM step, the core of ARFF training requires  a total number of epochs (n_epoch) and number of RWM steps (n_ω_steps).  The core of the training loop consists of:","category":"page"},{"location":"","page":"Home","title":"Home","text":"for i in 1:n_epochs\n    # solve for β with current ω\n    for j in 1:n_ω_steps\n        # generate an RWM proposal\n            for k in 1:K\n                # accept/reject each ω_k\n            end\n    end\nend","category":"page"},{"location":"#Acknowledgements","page":"Home","title":"Acknowledgements","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Contributors to this project include:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Gideon Simpson \nPetr Plechac\nJerome Troy\nLiam Doherty\nHunter Wages","category":"page"},{"location":"","page":"Home","title":"Home","text":"This work was supported in part by the ARO Cooperative Agreement W911NF2220234.","category":"page"},{"location":"#References","page":"Home","title":"References","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A. Kammonen, J. Kiessling, P. Plecháč, M. Sandberg and A. Szepessy. Adaptive random Fourier features with Metropolis sampling. Foundations of Data Science 2, 309–332 (2020). Accessed on Nov 18, 2023.\n\n\n\nJ. Kiessling, E. Ström and R. Tempone. Wind field reconstruction with adaptive random Fourier features. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences 477, 20210236 (2021). Accessed on May 5, 2023.\n\n\n\nA. Kammonen, J. Kiessling, P. Plecháč, M. Sandberg, A. Szepessy and R. Tempone. Smaller generalization error derived for a deep residual neural network compared with shallow networks. IMA Journal of Numerical Analysis 43, 2585–2632 (2023). Accessed on Nov 12, 2023.\n\n\n\n","category":"page"}]
}
