# Training with Resampling
To illustrate the use of resampling, we follow an example from Section 3 of 
[kammonen_adaptive_2024](@cite).  We will fit data generated by the following
function ``f: \mathbb{R}^d \to \mathbb{R}``:
```math
\begin{align*}
    \boldsymbol{x}_m &\sim \mathcal{N}(\boldsymbol{0}, Id), \\
    y_m &= f(\boldsymbol{x}_m), \\
    f(\boldsymbol{x}) &= \mathrm{Si}\left(\frac{[\boldsymbol{x}B]_1}{a}\right)\exp\left(-\frac{||\boldsymbol{x}B||_2^2}{2}\right),
\end{align*}
```
for row vector ``\boldsymbol{x}_m \in \mathbb{R}^{1 \times d}`` and where
``[\boldsymbol{x}B]_1`` denotes the first component of the vector
``\boldsymbol{x}B`` with ``B`` being the rotation matrix defined as follows:
```math
B = \begin{bmatrix}
0.8617 & 0.4975 & -0.0998 & -0.0000 \\
0.3028 & -0.5246 & -0.0000 & 0.7957 \\
0.0865 & 0.0499 & 0.9950 & 0.0000 \\
0.3978 & -0.6891 & -0.0000 & -0.6057 \\
\end{bmatrix}.
```
In our example, we will use $a = 10^{-2}$.

```@setup ex4
using Revise
using ARFF
using Random
using Statistics
using LinearAlgebra
using SpecialFunctions
using Plots
using LaTeXStrings
using Distributions
```

## Define Function
```@example ex4

Bt = [  0.8617 0.4975 -0.0998 -0.0;
        0.3028 -0.5246 -0.0000 0.7957;
        0.0865 0.0499 0.9950 0.0000;
        0.3978 -0.6891 -0.0000 -0.6057];

B = (Bt')|>collect;
b1 = B[1,:];

a = 1e-2;
f(x) = sinint(b1⋅x/ a) * exp(-0.5 * (norm(B*x,2)^2)); nothing;
```

## Generate data
Next, we generate our training data.
```@example ex4
dx = 4;
nx = 10^4;
Random.seed!(100) # for reproducibility
x = [randn(dx) for _ in 1:nx];
y = f.(x);
data = DataSet(x, y); nothing;
```

## Define Training Parameters
We now define our model and prepare for training.  Note that this example uses [sigmoid type activation functions](@ref activation).
```@example ex4
@show K = 2^8;
Random.seed!(200) # for reproducibility

F0 = FourierModel([1.0 for _ in 1:K], [10 * rand(dx) for _ in 1:K], SigmoidActivation)
δ = 0.1 # rwm step size
λ = 1e-1 # regularization
n_epochs = 10^3 # number of epochs
# number of rwm steps between full updates and resampling
n_rwm_steps = [5, 5, 1] 
# use 10% of the run for burn in
n_burn = n_epochs ÷ 10 

batch_size = 10^3; nothing;
```

## Define solver and resampler for each R value
We now define the functions we intend to use, including a distinct resampler
depending on the value of `R`:
```@example ex4
linear_solver! = (β, ω, x, y, S, epoch) -> solve_normal!(β, S, y, λ=λ) # define linear solver

rwm_sampler = [AdaptiveRWMSampler(F0, linear_solver!, n_rwm_steps_, n_burn, δ) for n_rwm_steps_ in n_rwm_steps]; #define rwm sampler for each R.

R_vals = [0.0, 0.75, 1.0] # effective sample size thresholds

mutate_rwm1!(F, x, y, S, n) = ARFF.rwm!(F, rwm_sampler[1], x, y, S, n) # mutate rwm for R=0.0
mutate_rwm2!(F, x, y, S, n) = ARFF.rwm!(F, rwm_sampler[2], x, y, S, n) # mutate rwm for R=0.75
mutate_rwm3!(F, x, y, S, n) = ARFF.rwm!(F, rwm_sampler[3], x, y, S, n) # mutate rwm for R=1.0

mutate_rwm_array = [mutate_rwm1!, mutate_rwm2!, mutate_rwm3!]

resampler! = [(F, x, y, S, epoch) -> ARFF.resample!(F, x, y, S, epoch, rwm_sampler[i].linear_solve!, R=R_vals[i]) for i in 1:3] # resampler for every R value

solver = [ARFFSolver(rwm_sampler[i].linear_solve!, mutate_rwm_array[i], resampler![i], mse_loss) for i in 1:3]; # solver for every resampler 
nothing;
```
## Train and Evaluate Results

### Train $R=0.0$ Network
This example corresponds to pure RWM; no resampling takes place.
```@example ex4
Random.seed!(1000) # for reproducibility
G1 = deepcopy(F0)
loss1 = train_arff!(G1,data, batch_size, solver[1], n_epochs, show_progress=false);nothing;
```

### Train $R=0.8$ Network
In this case, resampling takes place when the ESS falls beneath ``R \cdot K``.
```@example ex4
Random.seed!(1000) # for reproducibility
G2 = deepcopy(F0)
loss2 = train_arff!(G2, data, batch_size, solver[2], n_epochs, show_progress=false); nothing;
```

### Train $R=1.0$ Network
In this case, resampling takes place at every step.
```@example ex4
Random.seed!(1000) # for reproducibility
G3 = deepcopy(F0)
loss3 = train_arff!(G3, data, batch_size, solver[3], n_epochs, show_progress=false); nothing;
```

### Compare Loss Curves
```@example ex4
plot(1:n_epochs, loss1, xscale=:log10, yscale = :log10, label = L"R=0.0", legend = :bottomleft)
plot!(1:n_epochs, loss2, label = L"R=0.75")
plot!(1:n_epochs, loss3, label = L"R=1.0")
xlabel!("Epoch")
ylabel!("Loss")
```
The parameters can be tuned in the above example to get more accurate results.
We have found resampling method improves some problems, but performs similarily
to the ``R=0.0`` case in others. The performance appears to improve the most
when using large data sets with large ``K`` values.  See
[kammonen_adaptive_2024](@cite) for more examples.

